{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Artificial Intelligence, Programming and Robotics Artificial Intelligence Identify, research and use artificial intelligence and virtualization techniques of reality in addressing and seeking solutions to basic problems of society, valuing the ethical and inclusive principles applied. Programming Apply computational thinking in the analysis and resolution of basic and significant problems for students through software development. Robotics Assemble simple robotic systems, analyzing the responses they provide in their interaction with the environment and assessing their effectiveness in the face of the challenges posed.","title":"Artificial Intelligence, Programming and Robotics"},{"location":"index.html#artificial-intelligence-programming-and-robotics","text":"","title":"Artificial Intelligence, Programming and Robotics"},{"location":"index.html#artificial-intelligence","text":"Identify, research and use artificial intelligence and virtualization techniques of reality in addressing and seeking solutions to basic problems of society, valuing the ethical and inclusive principles applied.","title":"Artificial Intelligence"},{"location":"index.html#programming","text":"Apply computational thinking in the analysis and resolution of basic and significant problems for students through software development.","title":"Programming"},{"location":"index.html#robotics","text":"Assemble simple robotic systems, analyzing the responses they provide in their interaction with the environment and assessing their effectiveness in the face of the challenges posed.","title":"Robotics"},{"location":"contents/ai/activity-turing.html","text":"The Turing test: Can a computer pass for a human? - Alex Gendler What is consciousness? Can an (1) really think? Does the mind just consist of (2) in the brain, or is there some intangible spark at its core? For many, these have been vital considerations for the future of artificial intelligence. But British computer scientist Alan Turing decided to disregard all these questions in favor of a much simpler one: can a computer talk like a human? This question led to an idea for measuring artificial intelligence that would famously come to be known as the (3). In the 1950 paper, \"Computing Machinery and Intelligence,\" Turing proposed the following game. A human (4) has a text conversation with unseen players and evaluates their responses. To pass the test, a computer must be able to replace one of the players without substantially changing the results. In other words, a computer would be considered intelligent if its conversation couldn't be easily distinguished from a human's. Turing predicted that by the year 2000, machines with 100 megabytes of memory would be able to easily pass his test. But he may have jumped the gun. Even though today's computers have far more (5) than that, few have succeeded, and those that have done well focused more on finding clever ways to fool judges than using overwhelming (6). Though it was never subjected to a real test, the first program with some claim to success was called ELIZA. With only a fairly short and simple script, it managed to mislead many people by (7) a psychologist, encouraging them to talk more and reflecting their own questions back at them. Another early script PARRY took the opposite approach by (8) a paranoid schizophrenic who kept steering the conversation back to his own preprogrammed obsessions. Their success in fooling people highlighted one (9) of the test. Humans regularly attribute intelligence to a whole range of things that are not actually intelligent. Nonetheless, annual competitions like the Loebner Prize, have made the test more formal with judges knowing ahead of time that some of their conversation partners are machines. But while the quality has improved, many (10) programmers have used similar strategies to ELIZA and PARRY. 1997's winner Catherine could carry on amazingly focused and intelligent conversation, but mostly if the judge wanted to talk about Bill Clinton. And the more recent winner Eugene Goostman was given the persona of a 13-year-old Ukrainian boy, so judges interpreted its nonsequiturs and awkward grammar as language and culture barriers. Meanwhile, other programs like Cleverbot have taken a different approach by (11) analyzing huge (12) of real conversations to determine the best responses. Some also store memories of previous conversations in order to improve over time. But while Cleverbot's individual responses can sound incredibly human, its lack of a consistent (13) and inability to deal with brand new topics are a dead giveaway. Who in Turing's day could have (14) that today's computers would be able to pilot spacecraft, perform delicate surgeries, and solve massive equations, but still struggle with the most basic small talk? Human (15) turns out to be an amazingly complex phenomenon that can't be captured by even the largest dictionary. Chatbots can be baffled by simple pauses, like \"umm...\" or questions with no correct answer. And a simple (16) sentence, like, \"I took the juice out of the fridge and gave it to him, but forgot to check the date,\" requires a wealth of underlying (17) and intuition to parse. It turns out that (18) a human conversation takes more than just increasing memory and (19) power, and as we get closer to Turing's goal, we may have to deal with all those big questions about (20) after all.","title":"Activity 1. The Turing Test"},{"location":"contents/ai/activity-turing.html#the-turing-test-can-a-computer-pass-for-a-human-alex-gendler","text":"","title":"The Turing test: Can a computer pass for a human? - Alex Gendler"},{"location":"contents/ai/activity-turing.html#_1","text":"What is consciousness? Can an (1) really think? Does the mind just consist of (2) in the brain, or is there some intangible spark at its core? For many, these have been vital considerations for the future of artificial intelligence. But British computer scientist Alan Turing decided to disregard all these questions in favor of a much simpler one: can a computer talk like a human? This question led to an idea for measuring artificial intelligence that would famously come to be known as the (3). In the 1950 paper, \"Computing Machinery and Intelligence,\" Turing proposed the following game. A human (4) has a text conversation with unseen players and evaluates their responses. To pass the test, a computer must be able to replace one of the players without substantially changing the results. In other words, a computer would be considered intelligent if its conversation couldn't be easily distinguished from a human's. Turing predicted that by the year 2000, machines with 100 megabytes of memory would be able to easily pass his test. But he may have jumped the gun. Even though today's computers have far more (5) than that, few have succeeded, and those that have done well focused more on finding clever ways to fool judges than using overwhelming (6). Though it was never subjected to a real test, the first program with some claim to success was called ELIZA. With only a fairly short and simple script, it managed to mislead many people by (7) a psychologist, encouraging them to talk more and reflecting their own questions back at them. Another early script PARRY took the opposite approach by (8) a paranoid schizophrenic who kept steering the conversation back to his own preprogrammed obsessions. Their success in fooling people highlighted one (9) of the test. Humans regularly attribute intelligence to a whole range of things that are not actually intelligent. Nonetheless, annual competitions like the Loebner Prize, have made the test more formal with judges knowing ahead of time that some of their conversation partners are machines. But while the quality has improved, many (10) programmers have used similar strategies to ELIZA and PARRY. 1997's winner Catherine could carry on amazingly focused and intelligent conversation, but mostly if the judge wanted to talk about Bill Clinton. And the more recent winner Eugene Goostman was given the persona of a 13-year-old Ukrainian boy, so judges interpreted its nonsequiturs and awkward grammar as language and culture barriers. Meanwhile, other programs like Cleverbot have taken a different approach by (11) analyzing huge (12) of real conversations to determine the best responses. Some also store memories of previous conversations in order to improve over time. But while Cleverbot's individual responses can sound incredibly human, its lack of a consistent (13) and inability to deal with brand new topics are a dead giveaway. Who in Turing's day could have (14) that today's computers would be able to pilot spacecraft, perform delicate surgeries, and solve massive equations, but still struggle with the most basic small talk? Human (15) turns out to be an amazingly complex phenomenon that can't be captured by even the largest dictionary. Chatbots can be baffled by simple pauses, like \"umm...\" or questions with no correct answer. And a simple (16) sentence, like, \"I took the juice out of the fridge and gave it to him, but forgot to check the date,\" requires a wealth of underlying (17) and intuition to parse. It turns out that (18) a human conversation takes more than just increasing memory and (19) power, and as we get closer to Turing's goal, we may have to deal with all those big questions about (20) after all.","title":""},{"location":"contents/ai/equity-inclusion.html","text":"Equity and inclusion in AI systems. Biases in AI Introduction Artificial intelligence (AI) is becoming increasingly important in our daily lives, from voice assistants to self-driving cars. However, it's important to understand that AI systems can also perpetuate and even amplify biases found in the data it is trained on. In this lesson, we will explore the concept of equity and inclusion in AI systems and the potential for biases in AI. What is equity and inclusion in AI systems? Equity and inclusion in AI systems refers to the idea that everyone should have access to the benefits of AI, regardless of their background or characteristics. This means that AI systems should be designed and implemented in a way that is fair and does not discriminate against certain groups of people. What are biases in AI systems? Biases in AI systems occur when the data used to train the AI is not representative of the population it will be used on. For example, if an AI system is trained on data mostly from one racial group, it may not be able to accurately recognize or make predictions about people from other racial groups. This can lead to unfair and harmful outcomes, such as denying certain people access to certain services or opportunities. How can we prevent biases in AI systems? There are several steps that can be taken to prevent biases in AI systems, including: Using diverse data sets to train AI systems: This can help ensure that the AI system is not just learning from one group of people. Regularly testing and evaluating AI systems: This can help identify and address any biases that may exist in the system. Involving diverse perspectives in the development and design of AI systems: This can help ensure that the needs and concerns of all groups are taken into account. Transparency and explainability in AI systems: This can help people understand how the system is making decisions and identify any potential biases. Conclusion AI has the potential to greatly benefit society, but it's important to ensure that it is developed and implemented in a way that is fair and inclusive. By understanding the concept of equity and inclusion and the potential for biases in AI systems, we can take steps to prevent these biases and create AI that truly benefits everyone.","title":"Equity and inclusion in AI systems. Biases in AI"},{"location":"contents/ai/equity-inclusion.html#equity-and-inclusion-in-ai-systems-biases-in-ai","text":"","title":"Equity and inclusion in AI systems. Biases in AI"},{"location":"contents/ai/equity-inclusion.html#introduction","text":"Artificial intelligence (AI) is becoming increasingly important in our daily lives, from voice assistants to self-driving cars. However, it's important to understand that AI systems can also perpetuate and even amplify biases found in the data it is trained on. In this lesson, we will explore the concept of equity and inclusion in AI systems and the potential for biases in AI.","title":"Introduction"},{"location":"contents/ai/equity-inclusion.html#what-is-equity-and-inclusion-in-ai-systems","text":"Equity and inclusion in AI systems refers to the idea that everyone should have access to the benefits of AI, regardless of their background or characteristics. This means that AI systems should be designed and implemented in a way that is fair and does not discriminate against certain groups of people.","title":"What is equity and inclusion in AI systems?"},{"location":"contents/ai/equity-inclusion.html#what-are-biases-in-ai-systems","text":"Biases in AI systems occur when the data used to train the AI is not representative of the population it will be used on. For example, if an AI system is trained on data mostly from one racial group, it may not be able to accurately recognize or make predictions about people from other racial groups. This can lead to unfair and harmful outcomes, such as denying certain people access to certain services or opportunities.","title":"What are biases in AI systems?"},{"location":"contents/ai/equity-inclusion.html#how-can-we-prevent-biases-in-ai-systems","text":"There are several steps that can be taken to prevent biases in AI systems, including: Using diverse data sets to train AI systems: This can help ensure that the AI system is not just learning from one group of people. Regularly testing and evaluating AI systems: This can help identify and address any biases that may exist in the system. Involving diverse perspectives in the development and design of AI systems: This can help ensure that the needs and concerns of all groups are taken into account. Transparency and explainability in AI systems: This can help people understand how the system is making decisions and identify any potential biases.","title":"How can we prevent biases in AI systems?"},{"location":"contents/ai/equity-inclusion.html#conclusion","text":"AI has the potential to greatly benefit society, but it's important to ensure that it is developed and implemented in a way that is fair and inclusive. By understanding the concept of equity and inclusion and the potential for biases in AI systems, we can take steps to prevent these biases and create AI that truly benefits everyone.","title":"Conclusion"},{"location":"contents/ai/sensors-applications.html","text":"Sensors applications","title":"Sensors applications"},{"location":"contents/ai/sensors-applications.html#sensors-applications","text":"","title":"Sensors applications"},{"location":"contents/ai/sensors.html","text":"Sensors Artificial Intelligence Artificial Intelligence (AI) is a technology that allows machines to learn, understand, and process information like humans. One of the key components of AI systems is sensors. Sensors are devices that can detect and measure physical and environmental conditions such as temperature, light, sound, and movement. In this lesson, we will learn about the different types of sensors used in AI applications and the different ways in which they can be used. Types of sensors Optical sensors : These sensors detect light and are used in applications such as facial recognition, object detection, and image processing. Examples of optical sensors include cameras and lidar sensors. Temperature sensors: These sensors measure temperature and are used in applications such as climate control and food safety. Examples of temperature sensors include thermocouples and thermistors. Pressure sensors: These sensors measure pressure and are used in applications such as industrial automation, weather forecasting, and healthcare. Examples of pressure sensors include piezoelectric sensors and strain gauge sensors. Accelerometer sensors: These sensors measure acceleration and are used in applications such as motion detection, navigation, and gaming. Examples of accelerometer sensors include MEMS accelerometers and piezoelectric accelerometers. Gyroscopic sensors: These sensors measure angular velocity and are used in applications such as navigation, gaming, and robotics. Examples of gyroscopic sensors include MEMS gyroscopes and fiber optic gyroscopes. Magnetic sensors: These sensors measure magnetic fields and are used in applications such as navigation, industrial automation, and healthcare. Examples of magnetic sensors include Hall effect sensors and magnetoresistive sensors. Ultrasonic sensors: These sensors measure distance and are used in applications such as object detection, navigation, and industrial automation. Examples of ultrasonic sensors include sonar sensors and lidar sensors. Infrared sensors: These sensors detect infrared radiation and are used in applications such as temperature measurement, night vision, and gesture recognition. Examples of infrared sensors include thermopile sensors and pyroelectric sensors. Proximity sensors: These sensors detect the presence of objects and are used in applications such as gesture recognition, object detection, and access control. Examples of proximity sensors include infrared proximity sensors and ultrasonic proximity sensors. Light sensors: These sensors detect light and are used in applications such as light control, gesture recognition, and object detection. Examples of light sensors include photodiodes and phototransistors. Humidity sensors: These sensors measure humidity and are used in applications such as weather forecasting, agriculture, and healthcare. Examples of humidity sensors include capacitive humidity sensors and resistive humidity sensors. Gas sensors: These sensors detect the presence of gases and are used in applications such as environmental monitoring, industrial automation, and healthcare. Examples of gas sensors include electrochemical gas sensors and metal oxide gas sensors. Images Artificial Intelligence","title":"Sensors"},{"location":"contents/ai/sensors.html#sensors","text":"","title":"Sensors"},{"location":"contents/ai/sensors.html#artificial-intelligence","text":"Artificial Intelligence (AI) is a technology that allows machines to learn, understand, and process information like humans. One of the key components of AI systems is sensors. Sensors are devices that can detect and measure physical and environmental conditions such as temperature, light, sound, and movement. In this lesson, we will learn about the different types of sensors used in AI applications and the different ways in which they can be used.","title":"Artificial Intelligence"},{"location":"contents/ai/sensors.html#types-of-sensors","text":"Optical sensors : These sensors detect light and are used in applications such as facial recognition, object detection, and image processing. Examples of optical sensors include cameras and lidar sensors. Temperature sensors: These sensors measure temperature and are used in applications such as climate control and food safety. Examples of temperature sensors include thermocouples and thermistors. Pressure sensors: These sensors measure pressure and are used in applications such as industrial automation, weather forecasting, and healthcare. Examples of pressure sensors include piezoelectric sensors and strain gauge sensors. Accelerometer sensors: These sensors measure acceleration and are used in applications such as motion detection, navigation, and gaming. Examples of accelerometer sensors include MEMS accelerometers and piezoelectric accelerometers. Gyroscopic sensors: These sensors measure angular velocity and are used in applications such as navigation, gaming, and robotics. Examples of gyroscopic sensors include MEMS gyroscopes and fiber optic gyroscopes. Magnetic sensors: These sensors measure magnetic fields and are used in applications such as navigation, industrial automation, and healthcare. Examples of magnetic sensors include Hall effect sensors and magnetoresistive sensors. Ultrasonic sensors: These sensors measure distance and are used in applications such as object detection, navigation, and industrial automation. Examples of ultrasonic sensors include sonar sensors and lidar sensors. Infrared sensors: These sensors detect infrared radiation and are used in applications such as temperature measurement, night vision, and gesture recognition. Examples of infrared sensors include thermopile sensors and pyroelectric sensors. Proximity sensors: These sensors detect the presence of objects and are used in applications such as gesture recognition, object detection, and access control. Examples of proximity sensors include infrared proximity sensors and ultrasonic proximity sensors. Light sensors: These sensors detect light and are used in applications such as light control, gesture recognition, and object detection. Examples of light sensors include photodiodes and phototransistors. Humidity sensors: These sensors measure humidity and are used in applications such as weather forecasting, agriculture, and healthcare. Examples of humidity sensors include capacitive humidity sensors and resistive humidity sensors. Gas sensors: These sensors detect the presence of gases and are used in applications such as environmental monitoring, industrial automation, and healthcare. Examples of gas sensors include electrochemical gas sensors and metal oxide gas sensors.","title":"Types of sensors"},{"location":"contents/ai/sensors.html#images","text":"Artificial Intelligence","title":"Images"},{"location":"contents/ai/terms.html","text":"Artificial Intelligence (AI) terms AI-Powered Assistant An AI-powered assistant is a virtual assistant that uses artificial intelligence (AI) technology to understand and respond to user requests. AI-powered assistants can be used to perform tasks such as scheduling appointments, setting reminders, providing information, and answering questions. Artificial Intelligence Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to perform tasks that would typically require human intelligence, such as reasoning, learning, perception, and problem-solving. Artificial intelligence is everywhere and it's already making a huge impact on our lives. It's autocompleting texts on our cellphones, telling us which videos to watch on YouTube, beating us at video games, recognizing us in photos, ordering products in stores, driving cars, scheduling appointments, you get the idea. Today we're going to explain what AI can (and can't) do right now and explain how we got to where we are today. Chatbot A chatbot is an AI-powered computer program designed to simulate conversation with human users, typically through text messages or voice interactions. Chatbots can be used for customer service, sales, marketing, and other applications. By now most of us have interacted with a chatbot in one form or another, but exactly how do they work? Do chatbots only operate on websites, or are there other mediums that a chatbot can facilitate a conversation? And why would anyone want to use a chatbot? In this lightboard video, Morgan Carroll with IBM Cloud, answers these questions and many more as she walks through an example of Floral company using a chatbot and shows first hand what a chatbot is, how it works, and why you may want to use one for your business. Do you ever lay awake at night wondering what, exactly, a chatbot is? Or how chatbots work? Or even if bots will steal customer service representatives\u2019 jobs? Well, you can rest easy because we\u2019re going to answer all your questions. Computer Vision Computer Vision is a field of artificial intelligence that focuses on enabling machines to interpret and understand visual data from the world around them, such as images and videos. Today we\u2019re going to talk about how computers see. We\u2019ve long known that our digital cameras and smartphones can take incredibly detailed images, but taking pictures is not quite the same thing. For the past half-century, computer scientists have been working to help our computing devices understand the imagery they capture, leading to advancements everywhere, from tracking hands and whole bodies, biometrics to unlock our phones, and eventually giving autonomous cars the ability to understand their surroundings. Decision Tree A decision tree is a graphical representation of a decision-making process that uses a tree-like model of decisions and their possible consequences. Decision trees are often used in machine learning and artificial intelligence applications. Decision tree organizes a series rules in a tree structure. It is one of the most practical methods for non-parametric supervised learning. Our goal in this video is to demonstrate how to create a decision tree that predicts the value of a target by learning decision rules inferred from the training data. Deep Learning Deep learning is a subset of machine learning that involves training artificial neural networks with large amounts of data to perform complex tasks, such as image and speech recognition. Expert System An expert system is an AI-powered system that uses a knowledge base and reasoning algorithms to simulate the decision-making abilities of a human expert in a particular domain. Machine Learning Machine learning is a subset of artificial intelligence that involves training computer programs to learn from data and improve their performance on specific tasks over time, without being explicitly programmed to do so. In this video, you\u2019ll learn more about the evolution of machine learning and its impact on daily life. Narrow AI Narrow AI refers to artificial intelligence systems that are designed to perform a specific task or set of tasks, rather than exhibiting general intelligence. Natural Language Generation Natural Language Generation is a field of artificial intelligence that focuses on using machine learning algorithms to automatically generate natural language text from structured data or other sources. Natural Language Processing Natural Language Processing is a field of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language. We\u2019re going to talk about how computers understand speech and speak themselves. As computers play an increasing role in our daily lives there has been an growing demand for voice user interfaces, but speech is also terribly complicated. Vocabularies are diverse, sentence structures can often dictate the meaning of certain words, and computers also have to deal with accents, mispronunciations, and many common linguistic faux pas. The field of Natural Language Processing, or NLP, attempts to solve these problems, with a number of techniques we\u2019ll discuss today. And even though our virtual assistants like Siri, Alexa, Google Home, Bixby, and Cortana have come a long way from the first speech processing and synthesis models, there is still much room for improvement. Neural Network A neural network is a type of artificial intelligence algorithm that is modeled after the structure and function of the human brain. Neural networks are often used in deep learning applications. We're going to combine the artificial neuron we created last week into an artificial neural network. Artificial neural networks are better than other methods for more complicated tasks like image recognition, and the key to their success is their hidden layers. We'll talk about how the math of these networks work and how using many hidden layers allows us to do deep learning. Neural networks are really powerful at finding patterns in data which is why they've become one of the most dominant machine learning technologies used today. Reinforcement Learning Reinforcement learning is a type of machine learning that involves training an algorithm to make decisions based on feedback it receives from its environment. Supervised Learning Supervised learning is a type of machine learning that involves training an algorithm using labeled data, where the desired output is known. Today we\u2019re going to teach John Green Bot how to tell the difference between donuts and bagels using supervised learning! Supervised learning is the process of learning WITH training labels, and is the most widely used kind of learning with it comes to AI - helping with stuff like tagging photos on Facebook and filtering spam from your email. We\u2019re going to start small today and show how just a single neuron (or perceptron) is constructed, and explain the differences between precision and recall. Next week, we'll build our first neural network. Training Data Training data is a set of data used to train machine learning algorithms. Training data typically consists of input data and corresponding output data, which is used to teach the algorithm how to make predictions. Turing Test The Turing Test is a test of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. What is consciousness? Can an artificial machine really think? For many, these have been vital considerations for the future of artificial intelligence. But British computer scientist Alan Turing decided to disregard all these questions in favor of a much simpler one: Can a computer talk like a human? Alex Gendler describes the Turing test and details some of its surprising results. Lesson by Alex Gendler, animation by Patrick Smith. Unsupervised Learning Unsupervised learning is a type of machine learning that involves training an algorithm using unlabeled data, where the desired output is unknown. The algorithm must find patterns and relationships in the data on its own. We\u2019re moving on from artificial intelligence that needs training labels, called Supervised Learning, to Unsupervised Learning which is learning by finding patterns in the world. We\u2019ll focus on the performing unsupervised clustering, specifically K-means clustering, and show you how we can extract meaningful patterns from data even when you don't know where those patterns are.","title":"AI terms"},{"location":"contents/ai/terms.html#artificial-intelligence-ai-terms","text":"","title":"Artificial Intelligence (AI) terms"},{"location":"contents/ai/terms.html#ai-powered-assistant","text":"An AI-powered assistant is a virtual assistant that uses artificial intelligence (AI) technology to understand and respond to user requests. AI-powered assistants can be used to perform tasks such as scheduling appointments, setting reminders, providing information, and answering questions.","title":"AI-Powered Assistant"},{"location":"contents/ai/terms.html#artificial-intelligence","text":"Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to perform tasks that would typically require human intelligence, such as reasoning, learning, perception, and problem-solving. Artificial intelligence is everywhere and it's already making a huge impact on our lives. It's autocompleting texts on our cellphones, telling us which videos to watch on YouTube, beating us at video games, recognizing us in photos, ordering products in stores, driving cars, scheduling appointments, you get the idea. Today we're going to explain what AI can (and can't) do right now and explain how we got to where we are today.","title":"Artificial Intelligence"},{"location":"contents/ai/terms.html#chatbot","text":"A chatbot is an AI-powered computer program designed to simulate conversation with human users, typically through text messages or voice interactions. Chatbots can be used for customer service, sales, marketing, and other applications. By now most of us have interacted with a chatbot in one form or another, but exactly how do they work? Do chatbots only operate on websites, or are there other mediums that a chatbot can facilitate a conversation? And why would anyone want to use a chatbot? In this lightboard video, Morgan Carroll with IBM Cloud, answers these questions and many more as she walks through an example of Floral company using a chatbot and shows first hand what a chatbot is, how it works, and why you may want to use one for your business. Do you ever lay awake at night wondering what, exactly, a chatbot is? Or how chatbots work? Or even if bots will steal customer service representatives\u2019 jobs? Well, you can rest easy because we\u2019re going to answer all your questions.","title":"Chatbot"},{"location":"contents/ai/terms.html#computer-vision","text":"Computer Vision is a field of artificial intelligence that focuses on enabling machines to interpret and understand visual data from the world around them, such as images and videos. Today we\u2019re going to talk about how computers see. We\u2019ve long known that our digital cameras and smartphones can take incredibly detailed images, but taking pictures is not quite the same thing. For the past half-century, computer scientists have been working to help our computing devices understand the imagery they capture, leading to advancements everywhere, from tracking hands and whole bodies, biometrics to unlock our phones, and eventually giving autonomous cars the ability to understand their surroundings.","title":"Computer Vision"},{"location":"contents/ai/terms.html#decision-tree","text":"A decision tree is a graphical representation of a decision-making process that uses a tree-like model of decisions and their possible consequences. Decision trees are often used in machine learning and artificial intelligence applications. Decision tree organizes a series rules in a tree structure. It is one of the most practical methods for non-parametric supervised learning. Our goal in this video is to demonstrate how to create a decision tree that predicts the value of a target by learning decision rules inferred from the training data.","title":"Decision Tree"},{"location":"contents/ai/terms.html#deep-learning","text":"Deep learning is a subset of machine learning that involves training artificial neural networks with large amounts of data to perform complex tasks, such as image and speech recognition.","title":"Deep Learning"},{"location":"contents/ai/terms.html#expert-system","text":"An expert system is an AI-powered system that uses a knowledge base and reasoning algorithms to simulate the decision-making abilities of a human expert in a particular domain.","title":"Expert System"},{"location":"contents/ai/terms.html#machine-learning","text":"Machine learning is a subset of artificial intelligence that involves training computer programs to learn from data and improve their performance on specific tasks over time, without being explicitly programmed to do so. In this video, you\u2019ll learn more about the evolution of machine learning and its impact on daily life.","title":"Machine Learning"},{"location":"contents/ai/terms.html#narrow-ai","text":"Narrow AI refers to artificial intelligence systems that are designed to perform a specific task or set of tasks, rather than exhibiting general intelligence.","title":"Narrow AI"},{"location":"contents/ai/terms.html#natural-language-generation","text":"Natural Language Generation is a field of artificial intelligence that focuses on using machine learning algorithms to automatically generate natural language text from structured data or other sources.","title":"Natural Language Generation"},{"location":"contents/ai/terms.html#natural-language-processing","text":"Natural Language Processing is a field of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language. We\u2019re going to talk about how computers understand speech and speak themselves. As computers play an increasing role in our daily lives there has been an growing demand for voice user interfaces, but speech is also terribly complicated. Vocabularies are diverse, sentence structures can often dictate the meaning of certain words, and computers also have to deal with accents, mispronunciations, and many common linguistic faux pas. The field of Natural Language Processing, or NLP, attempts to solve these problems, with a number of techniques we\u2019ll discuss today. And even though our virtual assistants like Siri, Alexa, Google Home, Bixby, and Cortana have come a long way from the first speech processing and synthesis models, there is still much room for improvement.","title":"Natural Language Processing"},{"location":"contents/ai/terms.html#neural-network","text":"A neural network is a type of artificial intelligence algorithm that is modeled after the structure and function of the human brain. Neural networks are often used in deep learning applications. We're going to combine the artificial neuron we created last week into an artificial neural network. Artificial neural networks are better than other methods for more complicated tasks like image recognition, and the key to their success is their hidden layers. We'll talk about how the math of these networks work and how using many hidden layers allows us to do deep learning. Neural networks are really powerful at finding patterns in data which is why they've become one of the most dominant machine learning technologies used today.","title":"Neural Network"},{"location":"contents/ai/terms.html#reinforcement-learning","text":"Reinforcement learning is a type of machine learning that involves training an algorithm to make decisions based on feedback it receives from its environment.","title":"Reinforcement Learning"},{"location":"contents/ai/terms.html#supervised-learning","text":"Supervised learning is a type of machine learning that involves training an algorithm using labeled data, where the desired output is known. Today we\u2019re going to teach John Green Bot how to tell the difference between donuts and bagels using supervised learning! Supervised learning is the process of learning WITH training labels, and is the most widely used kind of learning with it comes to AI - helping with stuff like tagging photos on Facebook and filtering spam from your email. We\u2019re going to start small today and show how just a single neuron (or perceptron) is constructed, and explain the differences between precision and recall. Next week, we'll build our first neural network.","title":"Supervised Learning"},{"location":"contents/ai/terms.html#training-data","text":"Training data is a set of data used to train machine learning algorithms. Training data typically consists of input data and corresponding output data, which is used to teach the algorithm how to make predictions.","title":"Training Data"},{"location":"contents/ai/terms.html#turing-test","text":"The Turing Test is a test of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. What is consciousness? Can an artificial machine really think? For many, these have been vital considerations for the future of artificial intelligence. But British computer scientist Alan Turing decided to disregard all these questions in favor of a much simpler one: Can a computer talk like a human? Alex Gendler describes the Turing test and details some of its surprising results. Lesson by Alex Gendler, animation by Patrick Smith.","title":"Turing Test"},{"location":"contents/ai/terms.html#unsupervised-learning","text":"Unsupervised learning is a type of machine learning that involves training an algorithm using unlabeled data, where the desired output is unknown. The algorithm must find patterns and relationships in the data on its own. We\u2019re moving on from artificial intelligence that needs training labels, called Supervised Learning, to Unsupervised Learning which is learning by finding patterns in the world. We\u2019ll focus on the performing unsupervised clustering, specifically K-means clustering, and show you how we can extract meaningful patterns from data even when you don't know where those patterns are.","title":"Unsupervised Learning"},{"location":"contents/ai/the-turing-test.html","text":"The Turing Test The Turing test is a way to measure a machine's ability to think and understand like a human. It was proposed by Alan Turing, a famous computer scientist, in 1950. The test works by having a human judge talk to both a human and a machine through a computer screen or other means of communication. If the judge can't tell which one is the machine, then the machine is said to have passed the Turing test and is considered to have human-like intelligence. It's a way to test the intelligence of machines. Alan Turing Alan Turing was a British computer scientist and mathematician who lived in the 20th century. He is famous for his work on cracking the code used by the Germans during World War II, which helped the Allies win the war. He also proposed the Turing test, which is a way to measure a machine's ability to think and understand like a human.","title":"The Turing Test"},{"location":"contents/ai/the-turing-test.html#the-turing-test","text":"The Turing test is a way to measure a machine's ability to think and understand like a human. It was proposed by Alan Turing, a famous computer scientist, in 1950. The test works by having a human judge talk to both a human and a machine through a computer screen or other means of communication. If the judge can't tell which one is the machine, then the machine is said to have passed the Turing test and is considered to have human-like intelligence. It's a way to test the intelligence of machines.","title":"The Turing Test"},{"location":"contents/ai/the-turing-test.html#alan-turing","text":"Alan Turing was a British computer scientist and mathematician who lived in the 20th century. He is famous for his work on cracking the code used by the Germans during World War II, which helped the Allies win the war. He also proposed the Turing test, which is a way to measure a machine's ability to think and understand like a human.","title":"Alan Turing"},{"location":"contents/ai/types-of-sensors.html","text":"Types of sensors There are different type of sensors are available to choose from and the characteristics of sensors are used for determining the type of sensor to be used for particular application.","title":"Types of sensors"},{"location":"contents/ai/types-of-sensors.html#types-of-sensors","text":"There are different type of sensors are available to choose from and the characteristics of sensors are used for determining the type of sensor to be used for particular application.","title":"Types of sensors"},{"location":"contents/programming/flipcoin.html","text":"Programming Project: Coin Flip Simulation This animation displays a simulation of 500 coin tosses . As can be observed, the percentages of heads and tails are converging towards 50% , which is the theoretical probability of getting a head or a tail in a single toss. 0. Heads or Tails Obviously, we need coins for this simulation. Coin flipping, coin tossing, or heads or tails is the practice of throwing a coin in the air and checking which side is showing when it lands, in order to choose between two alternatives, heads or tails, sometimes used to resolve a dispute between two parties. 1. Definition of variables These are the variables that need to be defined in the simulation application: Heads or tails? result : This variable stores a random integer between 0 and 1, meaning result can have a value of 0 or 1 . We will assign the value 0 to \"heads\" and the value 1 to \"tails.\" Number of heads and tails heads : The variable heads stores the number of heads that have appeared so far. tails : The variable tails stores the number of tails that have appeared so far. Flips or tosses flips : The variable flips stores the number of heads and tails that have appeared so far, in other words, the number of coin tosses performed. nflips : The variable nflips stores the number of tosses the simulator will perform . Frequency and percetages f_heads : The variable f_heads stores the percentage of heads that have appeared so far. It is a numerical value. f_tails : The variable f_tails stores the percentage of tails that have appeared so far. It is a numerical value. p_heads : The variable p_heads stores the percentage of heads that have appeared so far followed by the \"%\" character . It is a text value displayed in the interface. p_tails : The variable p_tails stores the percentage of tails that have appeared so far followed by the \"%\" character . It is a text value displayed in the interface. 2. The Main Loop The structure of the main program should look like this: wait 2 sec flips = 0 heads = 0 tails = 0 repeat nflips result = random(0,1) flips = flips + 1 wait 0.2 sec In Scratch, you can build this main loop using the following instructions and control structures:","title":"The Simulator: Flip Coin Simulation"},{"location":"contents/programming/flipcoin.html#programming-project-coin-flip-simulation","text":"This animation displays a simulation of 500 coin tosses . As can be observed, the percentages of heads and tails are converging towards 50% , which is the theoretical probability of getting a head or a tail in a single toss.","title":"Programming Project: Coin Flip Simulation"},{"location":"contents/programming/flipcoin.html#0-heads-or-tails","text":"Obviously, we need coins for this simulation. Coin flipping, coin tossing, or heads or tails is the practice of throwing a coin in the air and checking which side is showing when it lands, in order to choose between two alternatives, heads or tails, sometimes used to resolve a dispute between two parties.","title":"0. Heads or Tails"},{"location":"contents/programming/flipcoin.html#1-definition-of-variables","text":"These are the variables that need to be defined in the simulation application: Heads or tails? result : This variable stores a random integer between 0 and 1, meaning result can have a value of 0 or 1 . We will assign the value 0 to \"heads\" and the value 1 to \"tails.\" Number of heads and tails heads : The variable heads stores the number of heads that have appeared so far. tails : The variable tails stores the number of tails that have appeared so far. Flips or tosses flips : The variable flips stores the number of heads and tails that have appeared so far, in other words, the number of coin tosses performed. nflips : The variable nflips stores the number of tosses the simulator will perform . Frequency and percetages f_heads : The variable f_heads stores the percentage of heads that have appeared so far. It is a numerical value. f_tails : The variable f_tails stores the percentage of tails that have appeared so far. It is a numerical value. p_heads : The variable p_heads stores the percentage of heads that have appeared so far followed by the \"%\" character . It is a text value displayed in the interface. p_tails : The variable p_tails stores the percentage of tails that have appeared so far followed by the \"%\" character . It is a text value displayed in the interface.","title":"1. Definition of variables"},{"location":"contents/programming/flipcoin.html#2-the-main-loop","text":"The structure of the main program should look like this: wait 2 sec flips = 0 heads = 0 tails = 0 repeat nflips result = random(0,1) flips = flips + 1 wait 0.2 sec In Scratch, you can build this main loop using the following instructions and control structures:","title":"2. The Main Loop"},{"location":"contents/programming/lawlargenumbers.html","text":"The Law of Large Numbers The Law of Large Numbers is a fundamental principle in probability theory that states that as the number of repetitions of a random experiment increases, the average of the results tends to approach the expected value. In this exciting Scratch programming project, you will have the opportunity to explore and visualize this concept through a coin toss simulation.","title":"The Law of Large Numbers"},{"location":"contents/programming/lawlargenumbers.html#the-law-of-large-numbers","text":"The Law of Large Numbers is a fundamental principle in probability theory that states that as the number of repetitions of a random experiment increases, the average of the results tends to approach the expected value. In this exciting Scratch programming project, you will have the opportunity to explore and visualize this concept through a coin toss simulation.","title":"The Law of Large Numbers"},{"location":"contents/programming/problem-modeling.html","text":"Interpretation of reality through problem modeling Problem modeling In programming, problem modeling is the process of representing a real-world problem in a way that can be solved by a computer program. This typically involves breaking down the problem into smaller, more manageable parts and identifying the relationships between those parts. The interpretation of reality The interpretation of reality through problem modeling is the process of understanding how the problem to be solved relates to the real-world situation that it is intended to address, and using that understanding to inform the design of the program. This can help ensure that the program correctly captures the relevant aspects of the problem and produces accurate and useful results.","title":"Interpretation of reality through problem modeling"},{"location":"contents/programming/problem-modeling.html#interpretation-of-reality-through-problem-modeling","text":"","title":"Interpretation of reality through problem modeling"},{"location":"contents/programming/problem-modeling.html#problem-modeling","text":"In programming, problem modeling is the process of representing a real-world problem in a way that can be solved by a computer program. This typically involves breaking down the problem into smaller, more manageable parts and identifying the relationships between those parts.","title":"Problem modeling"},{"location":"contents/programming/problem-modeling.html#the-interpretation-of-reality","text":"The interpretation of reality through problem modeling is the process of understanding how the problem to be solved relates to the real-world situation that it is intended to address, and using that understanding to inform the design of the program. This can help ensure that the program correctly captures the relevant aspects of the problem and produces accurate and useful results.","title":"The interpretation of reality"},{"location":"contents/robotics/actuators.html","text":"Actuators An actuator is a component of a robot that is responsible for creating movement or motion . In other words, an actuator is what makes a robot move, grab, lift, or perform any other physical action. There are different types of actuators used in robotics, depending on the purpose and design of the robot. Some common types of actuators include: Motors : These are electrical or mechanical devices that convert electrical energy into mechanical energy to create motion. Motors are commonly used in robots to drive wheels, move robot arms, or control the position of robot joints. Pneumatic cylinders : These are devices that use compressed air to create motion. Pneumatic cylinders are often used in industrial robots to move heavy loads or in applications where precision and speed are important. Hydraulic cylinders : Similar to pneumatic cylinders, hydraulic cylinders use fluid instead of air to create motion. They are often used in large industrial robots for heavy-duty applications. Solenoids : These are electromechanical devices that convert electrical energy into linear or rotary motion. Solenoids are commonly used in small robots for precise movements, such as opening and closing robot grippers. In summary, an actuator is a critical component of a robot that creates motion or movement . Different types of actuators are used depending on the application and design of the robot. By using actuators, robots can perform physical tasks and interact with their environment, making them useful for a wide range of applications, from manufacturing to healthcare. Glossary English Spanish Example Sentence (English) Actuators Actuadores \"The robot's actuators allowed it to perform precise movements and interact with objects.\" Applications Aplicaciones \"Robots have various applications in industries such as manufacturing, healthcare, and more.\" Arms Brazos \"The robot's multiple arms enabled it to manipulate objects with dexterity.\" Component Componente \"The actuator is an essential component of a robot's mechanism.\" Control Control \"The robot's actuators were operated by a sophisticated control system.\" Convert Convertir \"The motor converts electrical energy into mechanical energy for locomotion.\" Create Crear \"The hydraulic cylinder creates powerful motion to lift heavy loads.\" Critical Cr\u00edtico \"The actuator is a critical element for the robot's functionality.\" Design Dise\u00f1o \"The choice of actuators depends on the robot's design and intended tasks.\" Devices Dispositivos \"Motors, solenoids, and cylinders are common devices used as actuators.\" Drive Impulsar \"The motors drive the robot's wheels, propelling it forward.\" Electrical El\u00e9ctrico \"Solenoids are electrical devices that convert energy into motion.\" Energy Energ\u00eda \"Actuators require a power source to operate and convert energy into motion.\" Fluid Fluido \"Hydraulic cylinders use fluid to generate motion in the robot's limbs.\" Grab Agarrar \"The robot's gripper can grab and manipulate objects of different shapes and sizes.\" Heavy-duty Resistente \"Industrial robots equipped with hydraulic cylinders can handle heavy-duty tasks.\" Important Importante \"Precision and speed are important factors in choosing pneumatic cylinders for a robot.\" Interact Interactuar \"Robots can interact with humans and their surroundings using various actuators.\" Linear Lineal \"Solenoids can produce linear or rotary motion depending on the design.\" Mechanical Mec\u00e1nico \"Motors are mechanical devices that convert electrical energy into mechanical motion.\" Motion Movimiento \"Actuators are responsible for creating motion and enabling a robot's locomotion.\" Multiple M\u00faltiple \"The robot had multiple actuators that worked together for coordinated movements.\" Perform Realizar \"Actuators allow robots to perform complex tasks with precision and accuracy.\" Position Posici\u00f3n \"Motors can control the position of a robot's arm or joint with high accuracy.\"","title":"Actuators"},{"location":"contents/robotics/actuators.html#actuators","text":"An actuator is a component of a robot that is responsible for creating movement or motion . In other words, an actuator is what makes a robot move, grab, lift, or perform any other physical action. There are different types of actuators used in robotics, depending on the purpose and design of the robot. Some common types of actuators include: Motors : These are electrical or mechanical devices that convert electrical energy into mechanical energy to create motion. Motors are commonly used in robots to drive wheels, move robot arms, or control the position of robot joints. Pneumatic cylinders : These are devices that use compressed air to create motion. Pneumatic cylinders are often used in industrial robots to move heavy loads or in applications where precision and speed are important. Hydraulic cylinders : Similar to pneumatic cylinders, hydraulic cylinders use fluid instead of air to create motion. They are often used in large industrial robots for heavy-duty applications. Solenoids : These are electromechanical devices that convert electrical energy into linear or rotary motion. Solenoids are commonly used in small robots for precise movements, such as opening and closing robot grippers. In summary, an actuator is a critical component of a robot that creates motion or movement . Different types of actuators are used depending on the application and design of the robot. By using actuators, robots can perform physical tasks and interact with their environment, making them useful for a wide range of applications, from manufacturing to healthcare.","title":"Actuators"},{"location":"contents/robotics/actuators.html#glossary","text":"English Spanish Example Sentence (English) Actuators Actuadores \"The robot's actuators allowed it to perform precise movements and interact with objects.\" Applications Aplicaciones \"Robots have various applications in industries such as manufacturing, healthcare, and more.\" Arms Brazos \"The robot's multiple arms enabled it to manipulate objects with dexterity.\" Component Componente \"The actuator is an essential component of a robot's mechanism.\" Control Control \"The robot's actuators were operated by a sophisticated control system.\" Convert Convertir \"The motor converts electrical energy into mechanical energy for locomotion.\" Create Crear \"The hydraulic cylinder creates powerful motion to lift heavy loads.\" Critical Cr\u00edtico \"The actuator is a critical element for the robot's functionality.\" Design Dise\u00f1o \"The choice of actuators depends on the robot's design and intended tasks.\" Devices Dispositivos \"Motors, solenoids, and cylinders are common devices used as actuators.\" Drive Impulsar \"The motors drive the robot's wheels, propelling it forward.\" Electrical El\u00e9ctrico \"Solenoids are electrical devices that convert energy into motion.\" Energy Energ\u00eda \"Actuators require a power source to operate and convert energy into motion.\" Fluid Fluido \"Hydraulic cylinders use fluid to generate motion in the robot's limbs.\" Grab Agarrar \"The robot's gripper can grab and manipulate objects of different shapes and sizes.\" Heavy-duty Resistente \"Industrial robots equipped with hydraulic cylinders can handle heavy-duty tasks.\" Important Importante \"Precision and speed are important factors in choosing pneumatic cylinders for a robot.\" Interact Interactuar \"Robots can interact with humans and their surroundings using various actuators.\" Linear Lineal \"Solenoids can produce linear or rotary motion depending on the design.\" Mechanical Mec\u00e1nico \"Motors are mechanical devices that convert electrical energy into mechanical motion.\" Motion Movimiento \"Actuators are responsible for creating motion and enabling a robot's locomotion.\" Multiple M\u00faltiple \"The robot had multiple actuators that worked together for coordinated movements.\" Perform Realizar \"Actuators allow robots to perform complex tasks with precision and accuracy.\" Position Posici\u00f3n \"Motors can control the position of a robot's arm or joint with high accuracy.\"","title":"Glossary"},{"location":"contents/robotics/controllers.html","text":"Controller A controller is a device or system that manages and controls the behavior of a robot. It receives input from sensors and provides output to actuators to ensure the robot moves or behaves in a specific way. Controllers are essential for ensuring that a robot operates safely, efficiently, and effectively. They can be simple or complex, depending on the complexity of the robot and the tasks it is designed to perform. For example, a simple controller might be used to control the motion of a small robot that moves in a straight line, while a more complex controller might be used to manage the movements of a larger robot with multiple arms and sensors. There are different types of controllers used in robotics, such as: Microcontrollers : These are small computers that are embedded within the robot itself. They can receive input from sensors, process data, and provide output to actuators, all in real-time. Microcontrollers are often used in small, simple robots. Programmable Logic Controllers (PLCs) : These are specialized computers that are designed to control industrial machinery, including robots. They can manage multiple inputs and outputs, and are often used in large, complex robotic systems. Robot Operating System (ROS) : This is an open-source platform for programming robots. It provides a range of tools and libraries that enable developers to create and manage the behavior of robots, including controllers. In summary, a controller is a device or system that manages and controls the behavior of a robot. It receives input from sensors and provides output to actuators, ensuring that the robot moves and behaves in a specific way. Different types of controllers are used depending on the complexity of the robot and the tasks it is designed to perform. Glossary English Spanish Example Sentence (English) Actuators Actuadores \"The robot's actuators responded to the controller's commands, enabling precise movements.\" Behavior Comportamiento \"The controller dictated the robot's behavior , ensuring it followed the desired operating parameters.\" Complexity Complejidad \"The controller's design varied based on the robot's complexity and the tasks it needed to accomplish.\" Computers Computadoras \"Microcontrollers and PLCs are types of computers used as controllers in robotics.\" Control Control \"The controller's main function was to control the robot's actions and responses to the environment.\" Efficiency Eficiencia \"The advanced controller improved the robot's efficiency by optimizing its movements and power usage.\" Embedded Incorporado \"Microcontrollers are embedded within the robot's structure, enabling real-time control.\" Input Entrada \"The controller processed the input from various sensors to make informed decisions.\" Manage Gestionar \"The controller was responsible for managing the robot's operations and maintaining its performance.\" Microcontrollers Microcontroladores \"Small robots often rely on microcontrollers as their primary controllers due to their compact size.\" Movements Movimientos \"The controller coordinated the robot's movements with precision and smoothness.\" Multiple M\u00faltiple \"Complex robots with multiple arms and sensors required a sophisticated controller for coordinated control.\" Open-source C\u00f3digo abierto \"ROS, an open-source platform, provided flexible and accessible tools for robot behavior control.\" Output Salida \"The controller generated output signals to direct the actuators and influence the robot's behavior.\" Platform Plataforma \"ROS served as a powerful platform for developing and implementing robot controllers.\" Programmable Programable \"PLCs offered a programmable solution for controlling complex robotic systems in industrial settings.\" Real-time Tiempo real \"Microcontrollers processed sensor data and produced real-time control signals for immediate robot response.\" Receive Recibir \"The controller could receive and interpret signals from various sensors to make informed decisions.\" Robotic systems Sistemas rob\u00f3ticos \"PLCs were commonly used in large robotic systems for managing multiple actuators and sensors.\" Sensors Sensores \"The controller relied on sensors to gather information about the robot's environment and conditions.\" Simple Simple \"A simple controller sufficed for the basic motion control of a small line-following robot.\" Tasks Tareas \"The controller assigned specific tasks to different actuators, enabling coordinated robot behavior.\" Tools Herramientas \"ROS provided a rich set of","title":"Controllers"},{"location":"contents/robotics/controllers.html#controller","text":"A controller is a device or system that manages and controls the behavior of a robot. It receives input from sensors and provides output to actuators to ensure the robot moves or behaves in a specific way. Controllers are essential for ensuring that a robot operates safely, efficiently, and effectively. They can be simple or complex, depending on the complexity of the robot and the tasks it is designed to perform. For example, a simple controller might be used to control the motion of a small robot that moves in a straight line, while a more complex controller might be used to manage the movements of a larger robot with multiple arms and sensors. There are different types of controllers used in robotics, such as: Microcontrollers : These are small computers that are embedded within the robot itself. They can receive input from sensors, process data, and provide output to actuators, all in real-time. Microcontrollers are often used in small, simple robots. Programmable Logic Controllers (PLCs) : These are specialized computers that are designed to control industrial machinery, including robots. They can manage multiple inputs and outputs, and are often used in large, complex robotic systems. Robot Operating System (ROS) : This is an open-source platform for programming robots. It provides a range of tools and libraries that enable developers to create and manage the behavior of robots, including controllers. In summary, a controller is a device or system that manages and controls the behavior of a robot. It receives input from sensors and provides output to actuators, ensuring that the robot moves and behaves in a specific way. Different types of controllers are used depending on the complexity of the robot and the tasks it is designed to perform.","title":"Controller"},{"location":"contents/robotics/controllers.html#glossary","text":"English Spanish Example Sentence (English) Actuators Actuadores \"The robot's actuators responded to the controller's commands, enabling precise movements.\" Behavior Comportamiento \"The controller dictated the robot's behavior , ensuring it followed the desired operating parameters.\" Complexity Complejidad \"The controller's design varied based on the robot's complexity and the tasks it needed to accomplish.\" Computers Computadoras \"Microcontrollers and PLCs are types of computers used as controllers in robotics.\" Control Control \"The controller's main function was to control the robot's actions and responses to the environment.\" Efficiency Eficiencia \"The advanced controller improved the robot's efficiency by optimizing its movements and power usage.\" Embedded Incorporado \"Microcontrollers are embedded within the robot's structure, enabling real-time control.\" Input Entrada \"The controller processed the input from various sensors to make informed decisions.\" Manage Gestionar \"The controller was responsible for managing the robot's operations and maintaining its performance.\" Microcontrollers Microcontroladores \"Small robots often rely on microcontrollers as their primary controllers due to their compact size.\" Movements Movimientos \"The controller coordinated the robot's movements with precision and smoothness.\" Multiple M\u00faltiple \"Complex robots with multiple arms and sensors required a sophisticated controller for coordinated control.\" Open-source C\u00f3digo abierto \"ROS, an open-source platform, provided flexible and accessible tools for robot behavior control.\" Output Salida \"The controller generated output signals to direct the actuators and influence the robot's behavior.\" Platform Plataforma \"ROS served as a powerful platform for developing and implementing robot controllers.\" Programmable Programable \"PLCs offered a programmable solution for controlling complex robotic systems in industrial settings.\" Real-time Tiempo real \"Microcontrollers processed sensor data and produced real-time control signals for immediate robot response.\" Receive Recibir \"The controller could receive and interpret signals from various sensors to make informed decisions.\" Robotic systems Sistemas rob\u00f3ticos \"PLCs were commonly used in large robotic systems for managing multiple actuators and sensors.\" Sensors Sensores \"The controller relied on sensors to gather information about the robot's environment and conditions.\" Simple Simple \"A simple controller sufficed for the basic motion control of a small line-following robot.\" Tasks Tareas \"The controller assigned specific tasks to different actuators, enabling coordinated robot behavior.\" Tools Herramientas \"ROS provided a rich set of","title":"Glossary"},{"location":"contents/robotics/crashcourse_robotics_ai.html","text":"Robotics: Crash Course AI #11 Video #2: Robotics (Crash Course AI #11) Robots aren\u2019t like humans who can do a lot of different things. They\u2019re designed for very specific tasks like vacuuming our homes, assembling cars in a factory, or exploring the surface of other planets. So even though it may be a while before we have a general household robot that can do it all, robots are still really important because they can do some things incredibly well even better than humans. So today, we're going to take a look at the role of AI in overcoming three key challenges in the field of robotics: localization, planning, and manipulation.","title":"Robotics: Crash Course AI #11"},{"location":"contents/robotics/crashcourse_robotics_ai.html#robotics-crash-course-ai-11","text":"","title":"Robotics: Crash Course AI #11"},{"location":"contents/robotics/crashcourse_robotics_ai.html#video-2-robotics-crash-course-ai-11","text":"Robots aren\u2019t like humans who can do a lot of different things. They\u2019re designed for very specific tasks like vacuuming our homes, assembling cars in a factory, or exploring the surface of other planets. So even though it may be a while before we have a general household robot that can do it all, robots are still really important because they can do some things incredibly well even better than humans. So today, we're going to take a look at the role of AI in overcoming three key challenges in the field of robotics: localization, planning, and manipulation.","title":"Video #2: Robotics (Crash Course AI #11)"},{"location":"contents/robotics/crashcourse_robots.html","text":"Robots Video #1: Robots (Crash Course Computer Science #37) Today we're going to talk about robots! Robots are often thought as a technology of the future, but they're already here by the millions in the workplace, our homes, and pretty soon on the roads. We'll discuss the origins of robotics to its proliferation, and even look at some common control designs that were implemented to make them more useful in the workplace. Robots are often thought of as a menace or danger to society, and although there definitely is the propensity for malicious uses, robots also have the potential to drastically improve the world. Video #1 Transcription Hi, I\u2019m Carrie Anne, and welcome to CrashCourse Computer Science! Today we\u2019re going to talk about robots. The word \u201crobot\u201d was first used in a 1920 Czech play to denote artificial, humanoid characters. The word was derived from \u201crobota\u201d, the slavic-language word for a forced laborer, indicating peasants in compulsory service in feudal, nineteenth century Europe. But even a century later, it\u2019s still a common portrayal: mass-produced, efficient, tireless creatures that look human-esque, but are emotionless, indifferent to self-preservation and lack creativity. There are many definitions for robots, but in general, these are machines capable of carrying out a series of actions automatically, guided by computer control. How they look isn\u2019t part of the equation \u2013 robots can be industrial arms that spray paint cars, drones that fly, snake-like medical robots that assist surgeons, as well as humanoid robotic assistants. Although the term \u201crobot\u201d is sometimes applied to interactive virtual characters, it\u2019s more appropriate to call these \u201cbots\u201d, or even better, \u201cagents.\u201d That\u2019s because the term \u201crobot\u201d carries a physical connotation, a machine that lives in and acts on the real world. The more general idea of self-operating machines goes back even further than the 1920s. Many ancient inventors created mechanical devices that performed functions automatically, like keeping the time and striking bells on the hour. There are plenty of examples of automated animal and humanoid figures that would perform dances, sing songs, strike drums, and do other physical actions. These non-electrical and certainly non-electronic machines were called automatons. For instance, an early automaton created in 1739 by the Frenchman Jacques de Vaucanson was the Canard Digerateur or Digesting Duck, a machine in the shape of a duck that appeared to eat grain and then defecate. The first machines controlled by computers emerged in the late 1940s. These Computer Numerical Control, or CNC, machines could run programs that instructed a machine to perform a series of operations. This level of control also enabled the creation of new manufactured goods, like milling a complex propellor design out of a block of aluminum \u2013 something that was difficult to do using standard machine tools, and with tolerances too small to be done by hand. CNC machines were a huge boon to industry, not just due to increased capability and precision, but also in terms of reducing labor costs by automating human jobs \u2013 a topic we\u2019ll revisit in a later episode. The first commercial deployment was a programmable industrial robot called the Unimate, sold to General Motors in 1960 to lift hot pieces of metal from a die casting machine and stack them. This was the start of the robotics industry. Soon, robots were stacking pallets, welding parts, painting cars and much more. The first image that jumps to your mind is probably a humanoid robot, like we usually see in shows or movies. Sometimes they\u2019re our friends and colleagues, but more often, they\u2019re sinister, apathetic, and battle-hardened. We also tend to think of robots as a technology of the future. But the reality is: they\u2019re already here \u2013 by the millions \u2013 and they\u2019re our workmates, helping us to do things harder, better, faster, and stronger. For simple motions \u2013 like a robotic gripper that moves back and forth on a track \u2013 a robot can be instructed to move to a particular position, and it\u2019ll keep moving in that direction until the desired position is reached, at which point it\u2019ll stop. This behavior can be achieved through a simple control loop. First, sense the robot position. Are we there yet? Nope. So keep moving. Now sense position again. Are we there yet? Nope So keep moving. Now sense position again. Are we there yet? Nope, so keep moving. Are we there yet? Yes! So we can stop moving, and also please be quiet! Because we\u2019re trying to minimize the distance between the sensed position and the desired position, this control loop is, more specifically, a negative feedback loop. A negative feedback control loop has three key pieces. There\u2019s a sensor, that measures things in the real world, like water pressure, motor position, air temperature, or whatever you\u2019re trying to control. From this measurement, we calculate how far we are from where we want to be \u2013 the error. The error is then interpreted by a controller, which decides how to instruct the system to minimize that error. Then, the system acts on the world though pumps, motors, heating elements, and other physical actuators. In tightly controlled environments, simple control loops, like this, work OK. But in many real world applications, things are a tad more complicated. Imagine that our gripper is really heavy, and even when the control loop says to stop, momentum causes the gripper to overshoot the desired position. That would cause the control loop to take over again, this time backing the gripper up. A badly tuned control loop might overshoot and overshoot and overshoot, and maybe even wobble forever. To make matters worse, in real world settings, there are typically external and variable forces acting on a robot, like friction, wind and items of different weight. To handle this gracefully, more sophisticated control logic is needed. A widely used control-loop, feedback mechanism is a proportional\u2013integral\u2013derivative controller. That\u2019s a bit of a mouthful, so people call them PID controllers. These used to be mechanical devices, but now it\u2019s all done in software. Let\u2019s imagine a robot that delivers coffee. Its goal is to travel between customers at two meters per second, which has been determined to be the ideal speed that\u2019s both safe and expedient. Of course, the environment doesn\u2019t always cooperate. Sometimes there\u2019s wind, and sometimes there's uphills and downhills and all sorts of things that affect the speed of the robot. So, it\u2019s going to have to increase and decrease power to its motors to maintain the desired speed. Using the robot's speed sensor, we can keep track of its actual speed and plot that alongside its desired speed. PID controllers calculate three values from this data. First is the proportional value, which is the difference between the desired value and the actual value at the most recent instant in time or the present. This is what our simpler control loop used before. The bigger the gap between actual and desired, the harder you'll push towards your target. In other words, it\u2019s proportional control. Next, the integral value is computed, which is the sum of error over a window of time, like the last few seconds. This look back helps compensate for steady state errors, resulting from things like motoring up a long hill. If this value is large, it means proportional control is not enough, and we have to push harder still. Finally, there\u2019s the derivative value, which is the rate of change between the desired and actual values. This helps account for possible future error and is sometimes called \"anticipatory control\". For example, if you are screaming in towards your goal too fast, you\u2019ll need to ease up a little to prevent overshoot. These three values are summed together, with different relative weights, to produce a controller output that\u2019s passed to the system. PID controllers are everywhere, from the cruise control in your car to drones that automatically adjust their rotor speeds to maintain level flight, as well as more exotic robots, like this one that balances on a ball to move around. Advanced robots often require many control loops running in parallel, working together, managing everything from robot balance to limb position. Control loops are responsible for getting robot attributes like location to desired values. So, you may be wondering where these values come from. This is the responsibility of higher-level robot software, which plans and executes robot actions, like plotting a path around sensed obstacles, or breaking down physical tasks, like picking up a ball, into simple, sequential motions. Using these techniques, robots have racked up some impressive achievements \u2013 they\u2019ve been to the deepest depths of Earth\u2019s oceans and roved around on Mars for over a decade. But interestingly, lots of problems that are trivial for many humans have turned out to be devilishly difficult for robots: like walking on two legs, opening a door, picking up objects without crushing them, putting on a t-shirt, or petting a dog. These are tasks you may be able to do without thinking, but a supercomputer-powered robot fails at spectacularly. These sorts of tasks are all active areas of robotics research. Artificial intelligence techniques, which we discussed a few episodes ago, are perhaps the most promising avenue to overcome these challenges. For example, Google has been running an experiment with a series of robotic arms that spend their days moving miscellaneous objects from one box to another, learning from trial and error. After thousands of hours of practice, the robots had cut their error rate in half. Of course, unlike humans, robots can run twenty-four hours a day and practice with many arms at the same time. So, it may just be a matter of time until they become adept at grasping things. But, for the time being, toddlers can out-grasp them. One of the biggest and most visible robotic breakthroughs in recent years has been self-driving, autonomous cars. If you think about it, cars don\u2019t have too many system inputs \u2013 you can speed up or slow down, and you can steer left or right. The tough part is sensing lanes, reading signs, and anticipating and navigating traffic, pedestrians, bicyclists, and a whole host of obstacles. In addition to being studded with proximity sensors, these robotic vehicles heavily rely on Computer Vision algorithms, which we discussed in Episode 35. We\u2019re also seeing the emergence of very primitive androids \u2013 robots that look and act like humans. Arguably, we\u2019re not close on either of those goals, as they tend to look pretty weird and act even weirder. At least we\u2019ll always have Westworld. But anyway, these remain a tantalizing goal for roboticists that combine many computer science topics we\u2019ve touched on over the last few episodes, like artificial intelligence, computer vision, and natural language processing. As for why humans are so fascinated by creating artificial embodiments of ourselves...you\u2019ll have to go to Crash Course Philosophy for that. And for the foreseeable future, realistic androids will continue to be the stuff of science fiction. Militaries also have a great interest in robots \u2013 they\u2019re not only replaceable but can surpass humans in attributes like strength, endurance, attention, and accuracy. Bomb disposal robots and reconnaissance drones are fairly common today. But fully autonomous, armed-to-the-teeth robots are slowly appearing, like the Samsung SGR-A1 sentry gun deployed by South Korea. Robots with the intelligence and capability to take human lives are called lethal autonomous weapons. And they\u2019re widely considered a complex and thorny issue. Without doubt, these systems could save soldiers lives by taking them off the battlefield and out of harm\u2019s way. It might even discourage war altogether. Though it\u2019s worth noting that people said the same thing about dynamite and nuclear weapons. On the flip side, we might be creating ruthlessly efficient killing machines that don\u2019t apply human judgment or compassion to complex situations. And the fog of war is about as complex and murky as they come. These robots would be taking orders and executing them as efficiently as they can, and sometimes human orders turn out to be really bad. This debate is going to continue for a long time, and pundits on both sides will grow louder as robotic technology improves. It\u2019s also an old debate \u2013 the danger was obvious to science fiction writer Isaac Asimov, who introduced a fictional \u201cThree Laws of Robotics\u201d in his 1942 short story \"Runaround\". And then, later he added a zeroth rule. In short, it\u2019s a code of conduct or moral compass for robots \u2013 guiding them to do no harm, especially to humans. It\u2019s pretty inadequate for practical application and it leaves plenty of room for equivocation. But still, Asimov\u2019s laws inspired a ton of science fiction and academic discussion, and today there are whole conferences on robot ethics. Importantly, Asimov crafted his fictional rules as a way to push back on \u201cRobot as a Menace\u201d memes common in fiction from his childhood. These were stories where robots went off the rails, harming or even destroying their creators in the process. Asimov, on the other hand, envisioned robots as useful, reliable, and even lovable machines. And it\u2019s this duality I want to leave you thinking about today. Like many of the technologies we\u2019ve discussed throughout this series, there are benevolent and malicious uses. Our job is to carefully reflect on computing's potential and peril, and wield our inventive talents to improve the state of the world. And robots are one of the most potent reminders of this responsibility. I\u2019ll see you next week.","title":"Robots"},{"location":"contents/robotics/crashcourse_robots.html#robots","text":"","title":"Robots"},{"location":"contents/robotics/crashcourse_robots.html#video-1-robots-crash-course-computer-science-37","text":"Today we're going to talk about robots! Robots are often thought as a technology of the future, but they're already here by the millions in the workplace, our homes, and pretty soon on the roads. We'll discuss the origins of robotics to its proliferation, and even look at some common control designs that were implemented to make them more useful in the workplace. Robots are often thought of as a menace or danger to society, and although there definitely is the propensity for malicious uses, robots also have the potential to drastically improve the world.","title":"Video #1: Robots (Crash Course Computer Science #37)"},{"location":"contents/robotics/crashcourse_robots.html#video-1-transcription","text":"Hi, I\u2019m Carrie Anne, and welcome to CrashCourse Computer Science! Today we\u2019re going to talk about robots. The word \u201crobot\u201d was first used in a 1920 Czech play to denote artificial, humanoid characters. The word was derived from \u201crobota\u201d, the slavic-language word for a forced laborer, indicating peasants in compulsory service in feudal, nineteenth century Europe. But even a century later, it\u2019s still a common portrayal: mass-produced, efficient, tireless creatures that look human-esque, but are emotionless, indifferent to self-preservation and lack creativity. There are many definitions for robots, but in general, these are machines capable of carrying out a series of actions automatically, guided by computer control. How they look isn\u2019t part of the equation \u2013 robots can be industrial arms that spray paint cars, drones that fly, snake-like medical robots that assist surgeons, as well as humanoid robotic assistants. Although the term \u201crobot\u201d is sometimes applied to interactive virtual characters, it\u2019s more appropriate to call these \u201cbots\u201d, or even better, \u201cagents.\u201d That\u2019s because the term \u201crobot\u201d carries a physical connotation, a machine that lives in and acts on the real world. The more general idea of self-operating machines goes back even further than the 1920s. Many ancient inventors created mechanical devices that performed functions automatically, like keeping the time and striking bells on the hour. There are plenty of examples of automated animal and humanoid figures that would perform dances, sing songs, strike drums, and do other physical actions. These non-electrical and certainly non-electronic machines were called automatons. For instance, an early automaton created in 1739 by the Frenchman Jacques de Vaucanson was the Canard Digerateur or Digesting Duck, a machine in the shape of a duck that appeared to eat grain and then defecate. The first machines controlled by computers emerged in the late 1940s. These Computer Numerical Control, or CNC, machines could run programs that instructed a machine to perform a series of operations. This level of control also enabled the creation of new manufactured goods, like milling a complex propellor design out of a block of aluminum \u2013 something that was difficult to do using standard machine tools, and with tolerances too small to be done by hand. CNC machines were a huge boon to industry, not just due to increased capability and precision, but also in terms of reducing labor costs by automating human jobs \u2013 a topic we\u2019ll revisit in a later episode. The first commercial deployment was a programmable industrial robot called the Unimate, sold to General Motors in 1960 to lift hot pieces of metal from a die casting machine and stack them. This was the start of the robotics industry. Soon, robots were stacking pallets, welding parts, painting cars and much more. The first image that jumps to your mind is probably a humanoid robot, like we usually see in shows or movies. Sometimes they\u2019re our friends and colleagues, but more often, they\u2019re sinister, apathetic, and battle-hardened. We also tend to think of robots as a technology of the future. But the reality is: they\u2019re already here \u2013 by the millions \u2013 and they\u2019re our workmates, helping us to do things harder, better, faster, and stronger. For simple motions \u2013 like a robotic gripper that moves back and forth on a track \u2013 a robot can be instructed to move to a particular position, and it\u2019ll keep moving in that direction until the desired position is reached, at which point it\u2019ll stop. This behavior can be achieved through a simple control loop. First, sense the robot position. Are we there yet? Nope. So keep moving. Now sense position again. Are we there yet? Nope So keep moving. Now sense position again. Are we there yet? Nope, so keep moving. Are we there yet? Yes! So we can stop moving, and also please be quiet! Because we\u2019re trying to minimize the distance between the sensed position and the desired position, this control loop is, more specifically, a negative feedback loop. A negative feedback control loop has three key pieces. There\u2019s a sensor, that measures things in the real world, like water pressure, motor position, air temperature, or whatever you\u2019re trying to control. From this measurement, we calculate how far we are from where we want to be \u2013 the error. The error is then interpreted by a controller, which decides how to instruct the system to minimize that error. Then, the system acts on the world though pumps, motors, heating elements, and other physical actuators. In tightly controlled environments, simple control loops, like this, work OK. But in many real world applications, things are a tad more complicated. Imagine that our gripper is really heavy, and even when the control loop says to stop, momentum causes the gripper to overshoot the desired position. That would cause the control loop to take over again, this time backing the gripper up. A badly tuned control loop might overshoot and overshoot and overshoot, and maybe even wobble forever. To make matters worse, in real world settings, there are typically external and variable forces acting on a robot, like friction, wind and items of different weight. To handle this gracefully, more sophisticated control logic is needed. A widely used control-loop, feedback mechanism is a proportional\u2013integral\u2013derivative controller. That\u2019s a bit of a mouthful, so people call them PID controllers. These used to be mechanical devices, but now it\u2019s all done in software. Let\u2019s imagine a robot that delivers coffee. Its goal is to travel between customers at two meters per second, which has been determined to be the ideal speed that\u2019s both safe and expedient. Of course, the environment doesn\u2019t always cooperate. Sometimes there\u2019s wind, and sometimes there's uphills and downhills and all sorts of things that affect the speed of the robot. So, it\u2019s going to have to increase and decrease power to its motors to maintain the desired speed. Using the robot's speed sensor, we can keep track of its actual speed and plot that alongside its desired speed. PID controllers calculate three values from this data. First is the proportional value, which is the difference between the desired value and the actual value at the most recent instant in time or the present. This is what our simpler control loop used before. The bigger the gap between actual and desired, the harder you'll push towards your target. In other words, it\u2019s proportional control. Next, the integral value is computed, which is the sum of error over a window of time, like the last few seconds. This look back helps compensate for steady state errors, resulting from things like motoring up a long hill. If this value is large, it means proportional control is not enough, and we have to push harder still. Finally, there\u2019s the derivative value, which is the rate of change between the desired and actual values. This helps account for possible future error and is sometimes called \"anticipatory control\". For example, if you are screaming in towards your goal too fast, you\u2019ll need to ease up a little to prevent overshoot. These three values are summed together, with different relative weights, to produce a controller output that\u2019s passed to the system. PID controllers are everywhere, from the cruise control in your car to drones that automatically adjust their rotor speeds to maintain level flight, as well as more exotic robots, like this one that balances on a ball to move around. Advanced robots often require many control loops running in parallel, working together, managing everything from robot balance to limb position. Control loops are responsible for getting robot attributes like location to desired values. So, you may be wondering where these values come from. This is the responsibility of higher-level robot software, which plans and executes robot actions, like plotting a path around sensed obstacles, or breaking down physical tasks, like picking up a ball, into simple, sequential motions. Using these techniques, robots have racked up some impressive achievements \u2013 they\u2019ve been to the deepest depths of Earth\u2019s oceans and roved around on Mars for over a decade. But interestingly, lots of problems that are trivial for many humans have turned out to be devilishly difficult for robots: like walking on two legs, opening a door, picking up objects without crushing them, putting on a t-shirt, or petting a dog. These are tasks you may be able to do without thinking, but a supercomputer-powered robot fails at spectacularly. These sorts of tasks are all active areas of robotics research. Artificial intelligence techniques, which we discussed a few episodes ago, are perhaps the most promising avenue to overcome these challenges. For example, Google has been running an experiment with a series of robotic arms that spend their days moving miscellaneous objects from one box to another, learning from trial and error. After thousands of hours of practice, the robots had cut their error rate in half. Of course, unlike humans, robots can run twenty-four hours a day and practice with many arms at the same time. So, it may just be a matter of time until they become adept at grasping things. But, for the time being, toddlers can out-grasp them. One of the biggest and most visible robotic breakthroughs in recent years has been self-driving, autonomous cars. If you think about it, cars don\u2019t have too many system inputs \u2013 you can speed up or slow down, and you can steer left or right. The tough part is sensing lanes, reading signs, and anticipating and navigating traffic, pedestrians, bicyclists, and a whole host of obstacles. In addition to being studded with proximity sensors, these robotic vehicles heavily rely on Computer Vision algorithms, which we discussed in Episode 35. We\u2019re also seeing the emergence of very primitive androids \u2013 robots that look and act like humans. Arguably, we\u2019re not close on either of those goals, as they tend to look pretty weird and act even weirder. At least we\u2019ll always have Westworld. But anyway, these remain a tantalizing goal for roboticists that combine many computer science topics we\u2019ve touched on over the last few episodes, like artificial intelligence, computer vision, and natural language processing. As for why humans are so fascinated by creating artificial embodiments of ourselves...you\u2019ll have to go to Crash Course Philosophy for that. And for the foreseeable future, realistic androids will continue to be the stuff of science fiction. Militaries also have a great interest in robots \u2013 they\u2019re not only replaceable but can surpass humans in attributes like strength, endurance, attention, and accuracy. Bomb disposal robots and reconnaissance drones are fairly common today. But fully autonomous, armed-to-the-teeth robots are slowly appearing, like the Samsung SGR-A1 sentry gun deployed by South Korea. Robots with the intelligence and capability to take human lives are called lethal autonomous weapons. And they\u2019re widely considered a complex and thorny issue. Without doubt, these systems could save soldiers lives by taking them off the battlefield and out of harm\u2019s way. It might even discourage war altogether. Though it\u2019s worth noting that people said the same thing about dynamite and nuclear weapons. On the flip side, we might be creating ruthlessly efficient killing machines that don\u2019t apply human judgment or compassion to complex situations. And the fog of war is about as complex and murky as they come. These robots would be taking orders and executing them as efficiently as they can, and sometimes human orders turn out to be really bad. This debate is going to continue for a long time, and pundits on both sides will grow louder as robotic technology improves. It\u2019s also an old debate \u2013 the danger was obvious to science fiction writer Isaac Asimov, who introduced a fictional \u201cThree Laws of Robotics\u201d in his 1942 short story \"Runaround\". And then, later he added a zeroth rule. In short, it\u2019s a code of conduct or moral compass for robots \u2013 guiding them to do no harm, especially to humans. It\u2019s pretty inadequate for practical application and it leaves plenty of room for equivocation. But still, Asimov\u2019s laws inspired a ton of science fiction and academic discussion, and today there are whole conferences on robot ethics. Importantly, Asimov crafted his fictional rules as a way to push back on \u201cRobot as a Menace\u201d memes common in fiction from his childhood. These were stories where robots went off the rails, harming or even destroying their creators in the process. Asimov, on the other hand, envisioned robots as useful, reliable, and even lovable machines. And it\u2019s this duality I want to leave you thinking about today. Like many of the technologies we\u2019ve discussed throughout this series, there are benevolent and malicious uses. Our job is to carefully reflect on computing's potential and peril, and wield our inventive talents to improve the state of the world. And robots are one of the most potent reminders of this responsibility. I\u2019ll see you next week.","title":"Video #1 Transcription"},{"location":"contents/robotics/crashcourse_robots_engineering.html","text":"How Engineering Robots Works Video #3: How Engineering Robots Works (Crash Course Engineering #33) In this episode we looked at robots and the engineering principles of robots. We learned how robots use sensors to interpret their environment, how actuators and effectors allow a robot to manipulate the objects around it to accomplish a task, and how computers coordinate the efforts of the two.","title":"How Engineering Robots Works"},{"location":"contents/robotics/crashcourse_robots_engineering.html#how-engineering-robots-works","text":"","title":"How Engineering Robots Works"},{"location":"contents/robotics/crashcourse_robots_engineering.html#video-3-how-engineering-robots-works-crash-course-engineering-33","text":"In this episode we looked at robots and the engineering principles of robots. We learned how robots use sensors to interpret their environment, how actuators and effectors allow a robot to manipulate the objects around it to accomplish a task, and how computers coordinate the efforts of the two.","title":"Video #3: How Engineering Robots Works (Crash Course Engineering #33)"},{"location":"contents/robotics/intro_sens_act_contr.html","text":"Sensors, actuators and controllers Sensors are devices that detect or measure physical quantities such as temperature, light, pressure, and distance. In robotics, sensors are used to detect changes in the environment and provide feedback to the system. For example, a robot may use sensors to detect obstacles in its path or to measure the distance to an object. Actuators , on the other hand, are devices that are responsible for controlling motion or movement in a robot. They are used to make a robot move or perform a specific action based on the input received from the controller. Examples of actuators include motors, pneumatic cylinders, and solenoids. Controllers are devices or systems that manage and control the overall behavior of a robot. They process input from sensors and provide output to actuators to ensure the robot moves or behaves in a specific way. Controllers can be simple or complex, depending on the complexity of the robot and the tasks it is designed to perform. To summarize, sensors detect changes in the environment, actuators create motion or movement, and controllers manage the overall behavior of the robot. All three components work together to allow a robot to perform specific tasks and interact with its environment.","title":"Intro sens act contr"},{"location":"contents/robotics/intro_sens_act_contr.html#sensors-actuators-and-controllers","text":"Sensors are devices that detect or measure physical quantities such as temperature, light, pressure, and distance. In robotics, sensors are used to detect changes in the environment and provide feedback to the system. For example, a robot may use sensors to detect obstacles in its path or to measure the distance to an object. Actuators , on the other hand, are devices that are responsible for controlling motion or movement in a robot. They are used to make a robot move or perform a specific action based on the input received from the controller. Examples of actuators include motors, pneumatic cylinders, and solenoids. Controllers are devices or systems that manage and control the overall behavior of a robot. They process input from sensors and provide output to actuators to ensure the robot moves or behaves in a specific way. Controllers can be simple or complex, depending on the complexity of the robot and the tasks it is designed to perform. To summarize, sensors detect changes in the environment, actuators create motion or movement, and controllers manage the overall behavior of the robot. All three components work together to allow a robot to perform specific tasks and interact with its environment.","title":"Sensors, actuators and controllers"},{"location":"contents/robotics/mblock.html","text":"mBlock What Is mBlock 5? mBlock 5 is designed for the Science, Technology, Engineering, Arts and Mathematics (STEAM) education. Inspired by Scratch 3.0, it supports both graphical and textual programming languages. Currently, more than 10 million people are using it to learn programming, create their own projects, and share their creations. With mBlock 5, you can design engaging stories, games, and animations, and program devices such as Makeblock robots and microbit. On mBlock 5, you can switch to the Python mode simply with one-click. In addition, mBlock 5 integrates cutting-edge technologies including Artificial Intelligence (AI) and Internet of Things (IoT). Software versions Currently, the following versions are available: For PCs (software required): www.mblock.cc/en-us/download Web version (no software required): ide.mblock.cc/ For Android and iOS : Search for mBlock on any app store to download it","title":"mBlock"},{"location":"contents/robotics/mblock.html#mblock","text":"","title":"mBlock"},{"location":"contents/robotics/mblock.html#what-is-mblock-5","text":"mBlock 5 is designed for the Science, Technology, Engineering, Arts and Mathematics (STEAM) education. Inspired by Scratch 3.0, it supports both graphical and textual programming languages. Currently, more than 10 million people are using it to learn programming, create their own projects, and share their creations. With mBlock 5, you can design engaging stories, games, and animations, and program devices such as Makeblock robots and microbit. On mBlock 5, you can switch to the Python mode simply with one-click. In addition, mBlock 5 integrates cutting-edge technologies including Artificial Intelligence (AI) and Internet of Things (IoT).","title":"What Is mBlock 5?"},{"location":"contents/robotics/mblock.html#software-versions","text":"Currently, the following versions are available: For PCs (software required): www.mblock.cc/en-us/download Web version (no software required): ide.mblock.cc/ For Android and iOS : Search for mBlock on any app store to download it","title":"Software versions"},{"location":"contents/robotics/mbot.html","text":"mBot Ultrasonic Sensor Me Ultrasonic module is an electronic module designed for distance detection. The distance range it can detect is 3-400 cm. mBot can use this module to avoid obstacles or for other programs about distance detection. The yellow tag on the interface of this module indicates that it is a single digital interface and that it should be connected to a port with the yellow tag on the main control board. The ultrasonic sensor measures distance. One of the \u201ceyes\u201d transmits a sound, and the other waits for the echo of the sound to return. From the time this process takes, the distance of the object from the sensor can be calculated. The ultrasonic sensor has a range of 3-400cm. If an object is outside this range, the sensor will return a value of 400. Line Follower Sensors Me Line Follower is designed for line-following robots. There are two sensors, each with an IR transmitting LED and an IR static induction phototransistor. mBot can move along a black line on a white background or a white line on a black background. It features fast detection and simple circuits. The blue tag on the interface of this module indicates that it is a dual digital interface and that it should be connected to a port with the blue tag on the main control board. The line follower has 2 sensors which can detect a white surface (within the range of 1-2cm). It works by emitting IR (InfraRed) light and recording how much is reflected back: If a lot is reflected back, it can be deduced it is close to a white surface. If a little is reflected back, it can be deduced that the surface is black, or the sensor is not near a surface. A light surface reflects a lot of infrared light back to the receiver. A dark surface only reflects a little light back to the receiver. TT Geared Motor TT Geared Motor DC 6 V / 200 RPM is the new power source with plastic gears. The TT Geared Motor perfectly works with Makeblock Plastic Timing Pulley 62T and Plastic Timing Pulley 90T for the wheel systems of DIY projects. It can be used to power mBot.","title":"mBot"},{"location":"contents/robotics/mbot.html#mbot","text":"","title":"mBot"},{"location":"contents/robotics/mbot.html#ultrasonic-sensor","text":"Me Ultrasonic module is an electronic module designed for distance detection. The distance range it can detect is 3-400 cm. mBot can use this module to avoid obstacles or for other programs about distance detection. The yellow tag on the interface of this module indicates that it is a single digital interface and that it should be connected to a port with the yellow tag on the main control board. The ultrasonic sensor measures distance. One of the \u201ceyes\u201d transmits a sound, and the other waits for the echo of the sound to return. From the time this process takes, the distance of the object from the sensor can be calculated. The ultrasonic sensor has a range of 3-400cm. If an object is outside this range, the sensor will return a value of 400.","title":"Ultrasonic Sensor"},{"location":"contents/robotics/mbot.html#line-follower-sensors","text":"Me Line Follower is designed for line-following robots. There are two sensors, each with an IR transmitting LED and an IR static induction phototransistor. mBot can move along a black line on a white background or a white line on a black background. It features fast detection and simple circuits. The blue tag on the interface of this module indicates that it is a dual digital interface and that it should be connected to a port with the blue tag on the main control board. The line follower has 2 sensors which can detect a white surface (within the range of 1-2cm). It works by emitting IR (InfraRed) light and recording how much is reflected back: If a lot is reflected back, it can be deduced it is close to a white surface. If a little is reflected back, it can be deduced that the surface is black, or the sensor is not near a surface. A light surface reflects a lot of infrared light back to the receiver. A dark surface only reflects a little light back to the receiver.","title":"Line Follower Sensors"},{"location":"contents/robotics/mbot.html#tt-geared-motor","text":"TT Geared Motor DC 6 V / 200 RPM is the new power source with plastic gears. The TT Geared Motor perfectly works with Makeblock Plastic Timing Pulley 62T and Plastic Timing Pulley 90T for the wheel systems of DIY projects. It can be used to power mBot.","title":"TT Geared Motor"},{"location":"contents/robotics/mbotcode.html","text":"mBot Robot Programming Main code The Ultrasonic Sensor Me Ultrasonic module is an electronic module designed for distance detection. The distance range it can detect is 3-400 cm. mBot can use this module to avoid obstacles or for other programs about distance detection. The yellow tag on the interface of this module indicates that it is a single digital interface and that it should be connected to a port with the yellow tag on the main control board. The ultrasonic sensor measures distance. One of the \u201ceyes\u201d transmits a sound, and the other waits for the echo of the sound to return. From the time this process takes, the distance of the object from the sensor can be calculated. The ultrasonic sensor has a range of 3-400cm. If an object is outside this range, the sensor will return a value of 400. CheckObstacles block This block is used to check for obstacles in front of the mBot. It allows the program to detect if there is an obstacle in the path of the robot using its sensors, such as the ultrasonic sensor. MoveForward block This block commands the mBot to move forward in a straight line. TurnRandom block This block instructs the mBot to turn in a random direction. It can be used to introduce randomness in the robot's movement or to create unpredictable behaviors. The Line Follower The line follower has 2 sensors which can detect a white surface (within the range of 1-2cm). It works by emitting IR (InfraRed) light and recording how much is reflected back: If a lot is reflected back, it can be deduced it is close to a white surface. If a little is reflected back, it can be deduced that the surface is black, or the sensor is not near a surface. FollowPath block This block instructs the mBot to follow a line using its line-following sensors. The block typically includes parameters or inputs to specify the speed or power at which the robot should move and the behavior it should exhibit when encountering different line configurations. The line-following sensors on the mBot detect the contrast between a line and the surrounding surface. The robot uses this information to determine its position relative to the line and adjust its movement accordingly. The mBot_FollowPath block allows users to define the specific actions the robot should take to stay on the line, such as turning left or right, stopping, or adjusting its speed.","title":"mBot Robot Programming"},{"location":"contents/robotics/mbotcode.html#mbot-robot-programming","text":"","title":"mBot Robot Programming"},{"location":"contents/robotics/mbotcode.html#main-code","text":"","title":"Main code"},{"location":"contents/robotics/mbotcode.html#the-ultrasonic-sensor","text":"Me Ultrasonic module is an electronic module designed for distance detection. The distance range it can detect is 3-400 cm. mBot can use this module to avoid obstacles or for other programs about distance detection. The yellow tag on the interface of this module indicates that it is a single digital interface and that it should be connected to a port with the yellow tag on the main control board. The ultrasonic sensor measures distance. One of the \u201ceyes\u201d transmits a sound, and the other waits for the echo of the sound to return. From the time this process takes, the distance of the object from the sensor can be calculated. The ultrasonic sensor has a range of 3-400cm. If an object is outside this range, the sensor will return a value of 400.","title":"The Ultrasonic Sensor"},{"location":"contents/robotics/mbotcode.html#checkobstacles-block","text":"This block is used to check for obstacles in front of the mBot. It allows the program to detect if there is an obstacle in the path of the robot using its sensors, such as the ultrasonic sensor.","title":"CheckObstacles block"},{"location":"contents/robotics/mbotcode.html#moveforward-block","text":"This block commands the mBot to move forward in a straight line.","title":"MoveForward block"},{"location":"contents/robotics/mbotcode.html#turnrandom-block","text":"This block instructs the mBot to turn in a random direction. It can be used to introduce randomness in the robot's movement or to create unpredictable behaviors.","title":"TurnRandom block"},{"location":"contents/robotics/mbotcode.html#the-line-follower","text":"The line follower has 2 sensors which can detect a white surface (within the range of 1-2cm). It works by emitting IR (InfraRed) light and recording how much is reflected back: If a lot is reflected back, it can be deduced it is close to a white surface. If a little is reflected back, it can be deduced that the surface is black, or the sensor is not near a surface.","title":"The Line Follower"},{"location":"contents/robotics/mbotcode.html#followpath-block","text":"This block instructs the mBot to follow a line using its line-following sensors. The block typically includes parameters or inputs to specify the speed or power at which the robot should move and the behavior it should exhibit when encountering different line configurations. The line-following sensors on the mBot detect the contrast between a line and the surrounding surface. The robot uses this information to determine its position relative to the line and adjust its movement accordingly. The mBot_FollowPath block allows users to define the specific actions the robot should take to stay on the line, such as turning left or right, stopping, or adjusting its speed.","title":"FollowPath block"},{"location":"contents/robotics/robotresearch.html","text":"Project: Researching an Advanced and Innovative Robot In this activity, you will work in pairs to research a specific advanced and innovative robot. You will gather information about the robot and create a presentation to share with the class. Part 1: Choosing a robot Start by discussing advanced and innovative robots with your partner. Choose a specific robot to research. Try to choose a robot that you find interesting or that relates to your personal interests. Part 2: Researching the robot Use reliable sources such as academic journals, reputable websites, and news articles to gather information about the robot you have chosen. Gather information on the following aspects of the robot: Purpose : What is the robot designed to do? Design : What does the robot look like? What components make up the robot? Sensors : What sensors does the robot use to interact with its environment? Actuators : What actuators does the robot use to move and interact with its environment? Controller : What type of controller does the robot use to manage its behavior? Innovations : What makes this robot advanced and innovative? What sets it apart from other robots? Applications : What are some of the potential applications for this robot? How might it be used in industry or everyday life? Part 3: Creating a presentation Use PowerPoint to create a presentation to share your research with the class. Your presentation should include the following: Introduction : Introduce the robot and explain why you chose it. Purpose : Describe what the robot is designed to do. Design : Share images and descriptions of the robot's design and components. Sensors : Explain what sensors the robot uses and how they work. Actuators : Explain what actuators the robot uses and how they work. Controller : Describe the type of controller the robot uses and how it manages the robot's behavior. Innovations : Discuss what makes this robot advanced and innovative. Applications : Share some potential applications for the robot. Conclusion : Summarize the key points and why this robot is interesting or important. Part 4: Presenting to the class Present your research to the class, using your presentation. Be prepared to answer questions from your classmates about your robot. This activity will allow you to explore and learn about advanced and innovative robots, while also developing your research and presentation skills. Have fun!","title":"Robotresearch"},{"location":"contents/robotics/robotresearch.html#project-researching-an-advanced-and-innovative-robot","text":"In this activity, you will work in pairs to research a specific advanced and innovative robot. You will gather information about the robot and create a presentation to share with the class.","title":"Project: Researching an Advanced and Innovative Robot"},{"location":"contents/robotics/robotresearch.html#part-1-choosing-a-robot","text":"Start by discussing advanced and innovative robots with your partner. Choose a specific robot to research. Try to choose a robot that you find interesting or that relates to your personal interests.","title":"Part 1: Choosing a robot"},{"location":"contents/robotics/robotresearch.html#part-2-researching-the-robot","text":"Use reliable sources such as academic journals, reputable websites, and news articles to gather information about the robot you have chosen. Gather information on the following aspects of the robot: Purpose : What is the robot designed to do? Design : What does the robot look like? What components make up the robot? Sensors : What sensors does the robot use to interact with its environment? Actuators : What actuators does the robot use to move and interact with its environment? Controller : What type of controller does the robot use to manage its behavior? Innovations : What makes this robot advanced and innovative? What sets it apart from other robots? Applications : What are some of the potential applications for this robot? How might it be used in industry or everyday life?","title":"Part 2: Researching the robot"},{"location":"contents/robotics/robotresearch.html#part-3-creating-a-presentation","text":"Use PowerPoint to create a presentation to share your research with the class. Your presentation should include the following: Introduction : Introduce the robot and explain why you chose it. Purpose : Describe what the robot is designed to do. Design : Share images and descriptions of the robot's design and components. Sensors : Explain what sensors the robot uses and how they work. Actuators : Explain what actuators the robot uses and how they work. Controller : Describe the type of controller the robot uses and how it manages the robot's behavior. Innovations : Discuss what makes this robot advanced and innovative. Applications : Share some potential applications for the robot. Conclusion : Summarize the key points and why this robot is interesting or important.","title":"Part 3: Creating a presentation"},{"location":"contents/robotics/robotresearch.html#part-4-presenting-to-the-class","text":"Present your research to the class, using your presentation. Be prepared to answer questions from your classmates about your robot. This activity will allow you to explore and learn about advanced and innovative robots, while also developing your research and presentation skills. Have fun!","title":"Part 4: Presenting to the class"},{"location":"contents/robotics/robots.html","text":"Robots What is a robot? A robot is a machine that can be programmed to perform a variety of tasks. These tasks can range from simple actions like moving objects from one place to another, to more complex actions like assembling parts or performing surgery. Robots can be designed to operate autonomously or be controlled by a human operator. What sets robots apart from other machines is their ability to sense their environment and respond to it. They do this through the use of sensors such as cameras, microphones, and touch sensors. Based on the information they receive from these sensors, robots can make decisions about what actions to take. Robots come in many different shapes and sizes, and can be designed for a variety of purposes. For example, some robots are designed to work in factories, while others are used in healthcare settings to assist with surgeries or help patients with mobility issues. As you study robotics, you'll learn more about the different types of robots and how they are used in various industries. Overall, robots are an exciting and rapidly developing field in technology, and studying robotics can lead to many interesting career opportunities. Glossary English Spanish Example Sentence (English) Robots Robots \" Robots are used in manufacturing to automate repetitive tasks.\" Machine M\u00e1quina \"The robotic machine assembled the components with precision.\" Programmed Programado/a \"The robot was programmed to navigate through a maze.\" Tasks Tareas \"The robot was assigned multiple tasks in the industrial setting.\" Actions Acciones \"Robots can perform complex actions such as grasping and manipulating objects.\" Moving Mover \"The robot is capable of moving heavy objects from one location to another.\" Objects Objetos \"The robot detected and identified objects using its vision system.\" Place Lugar \"The robot placed the finished product on the conveyor belt.\" Assembling Ensamblaje \"The robot is responsible for assembling electronic components in the production line.\" Parts Partes \"The robotic arm picked up the small parts and assembled them together.\" Performing Realizar \"The surgical robot is capable of performing precise and delicate procedures.\" Surgery Cirug\u00eda \"Robotic surgery offers the advantages of minimally invasive procedures.\" Operate Operar \"The robot can operate autonomously or be controlled by a human operator.\" Autonomously Aut\u00f3nomamente \"The autonomous robot navigated through the obstacle course without human intervention.\" Controlled Controlado/a \"The robot was controlled remotely by a skilled operator.\" Human operator Operador humano \"The robot collaborated with the human operator to perform complex tasks.\" Environment Entorno \"Robots use sensors to perceive and interact with their environment .\" Sensors Sensores \"The robot's sensors detected the presence of obstacles in its path.\" Cameras C\u00e1maras \"The robot's camera captured high-resolution images for visual inspection.\" Microphones Micr\u00f3fonos \"The robot's microphones picked up sound signals for voice recognition.\" Touch sensors Sensores t\u00e1ctiles \"The robot's touch sensors enabled it to detect and respond to tactile stimuli.\" Decisions Decisiones \"Based on sensor data, the robot made intelligent decisions to navigate its surroundings.\" Shapes Formas \"The robot's gripper was designed to handle objects of various shapes and sizes.\" Sizes Tama\u00f1os \"The robot adjusted its grip strength based on the size of the object it handled.\" Purposes Prop\u00f3sitos \"Robots are used in various industries for different purposes , such as manufacturing and healthcare.\"","title":"Robots"},{"location":"contents/robotics/robots.html#robots","text":"","title":"Robots"},{"location":"contents/robotics/robots.html#what-is-a-robot","text":"A robot is a machine that can be programmed to perform a variety of tasks. These tasks can range from simple actions like moving objects from one place to another, to more complex actions like assembling parts or performing surgery. Robots can be designed to operate autonomously or be controlled by a human operator. What sets robots apart from other machines is their ability to sense their environment and respond to it. They do this through the use of sensors such as cameras, microphones, and touch sensors. Based on the information they receive from these sensors, robots can make decisions about what actions to take. Robots come in many different shapes and sizes, and can be designed for a variety of purposes. For example, some robots are designed to work in factories, while others are used in healthcare settings to assist with surgeries or help patients with mobility issues. As you study robotics, you'll learn more about the different types of robots and how they are used in various industries. Overall, robots are an exciting and rapidly developing field in technology, and studying robotics can lead to many interesting career opportunities.","title":"What is a robot?"},{"location":"contents/robotics/robots.html#glossary","text":"English Spanish Example Sentence (English) Robots Robots \" Robots are used in manufacturing to automate repetitive tasks.\" Machine M\u00e1quina \"The robotic machine assembled the components with precision.\" Programmed Programado/a \"The robot was programmed to navigate through a maze.\" Tasks Tareas \"The robot was assigned multiple tasks in the industrial setting.\" Actions Acciones \"Robots can perform complex actions such as grasping and manipulating objects.\" Moving Mover \"The robot is capable of moving heavy objects from one location to another.\" Objects Objetos \"The robot detected and identified objects using its vision system.\" Place Lugar \"The robot placed the finished product on the conveyor belt.\" Assembling Ensamblaje \"The robot is responsible for assembling electronic components in the production line.\" Parts Partes \"The robotic arm picked up the small parts and assembled them together.\" Performing Realizar \"The surgical robot is capable of performing precise and delicate procedures.\" Surgery Cirug\u00eda \"Robotic surgery offers the advantages of minimally invasive procedures.\" Operate Operar \"The robot can operate autonomously or be controlled by a human operator.\" Autonomously Aut\u00f3nomamente \"The autonomous robot navigated through the obstacle course without human intervention.\" Controlled Controlado/a \"The robot was controlled remotely by a skilled operator.\" Human operator Operador humano \"The robot collaborated with the human operator to perform complex tasks.\" Environment Entorno \"Robots use sensors to perceive and interact with their environment .\" Sensors Sensores \"The robot's sensors detected the presence of obstacles in its path.\" Cameras C\u00e1maras \"The robot's camera captured high-resolution images for visual inspection.\" Microphones Micr\u00f3fonos \"The robot's microphones picked up sound signals for voice recognition.\" Touch sensors Sensores t\u00e1ctiles \"The robot's touch sensors enabled it to detect and respond to tactile stimuli.\" Decisions Decisiones \"Based on sensor data, the robot made intelligent decisions to navigate its surroundings.\" Shapes Formas \"The robot's gripper was designed to handle objects of various shapes and sizes.\" Sizes Tama\u00f1os \"The robot adjusted its grip strength based on the size of the object it handled.\" Purposes Prop\u00f3sitos \"Robots are used in various industries for different purposes , such as manufacturing and healthcare.\"","title":"Glossary"},{"location":"contents/robotics/robots_ai.html","text":"Robots and AI Robots and Artificial Intelligence Robots and artificial intelligence (AI) are closely related, but they are not the same thing. A robot is a machine that can be programmed to perform a variety of tasks, while AI refers to the ability of machines to perform tasks that would normally require human intelligence, such as learning, problem solving, and decision making. In many cases, robots use AI to make decisions about what actions to take. For example, a robot in a manufacturing plant might use AI to decide which parts to pick up and assemble based on the shape and size of the parts. Similarly, a robot in a healthcare setting might use AI to analyze medical images and make recommendations to doctors. However, not all robots use AI. Some robots are programmed to perform a specific set of tasks without any decision making capabilities. For example, a robot used to move heavy objects in a warehouse might simply follow a predetermined path and pick up objects along the way. On the other hand, not all AI is used in robots. AI can be used in many different applications, such as virtual assistants, self-driving cars, and fraud detection systems. These applications do not necessarily involve physical robots. Overall, the relationship between robots and AI is complex and evolving. As you study robotics and AI, you'll learn more about how these technologies are used together and separately to solve real-world problems. Glossary English Spanish Example Sentence (English) Robots Robots \" Robots are used in manufacturing to automate repetitive tasks.\" Artificial Intelligence Inteligencia Artificial \" Artificial Intelligence enables machines to learn, reason, and make decisions.\" Closely related Estrechamente relacionados/as \" Robots and artificial intelligence are closely related in the field of robotics.\" Not the same thing No es lo mismo \" Robots and AI are not the same thing , although they have connections.\" Machine M\u00e1quina \"The robotic machine performed complex tasks with precision.\" Programmed Programado/a \"The robot was programmed to follow a specific set of instructions.\" Variety of tasks Variedad de tareas \"Robots can be programmed to perform a variety of tasks in different industries.\" Learning Aprendizaje \"Artificial Intelligence involves learning from data to improve performance.\" Problem solving Resoluci\u00f3n de problemas \"AI algorithms are capable of problem solving in various domains.\" Decision making Toma de decisiones \"Robots equipped with AI can make intelligent decision making based on sensor data.\" Manufacturing plant Planta de manufactura \"The manufacturing plant used robots to automate the assembly line.\" Parts Partes \"The robot identified and sorted the different parts in the production process.\" Shape and size Forma y tama\u00f1o \"The robot used AI to determine the appropriate shape and size of the objects to handle.\" Healthcare setting Entorno sanitario \"Robots with AI are being used in healthcare settings to assist doctors and nurses.\" Analyze Analizar \"The robot analyzed the medical images to detect anomalies and abnormalities.\" Recommendations Recomendaciones \"Based on AI analysis, the robot provided accurate recommendations for treatment options.\" Decision making capabilities Capacidad de toma de decisiones \"This robot has advanced decision making capabilities based on its AI algorithms.\" Warehouse Almac\u00e9n \"The robot efficiently navigated the warehouse to retrieve and store items.\" Predetermined path Ruta predeterminada \"The robot followed a predetermined path to perform its tasks in the factory.\" Virtual assistants Asistentes virtuales \"Virtual assistants use AI to interact with users and provide helpful information.\" Self-driving cars Coches aut\u00f3nomos \"AI technology is driving the development of self-driving cars .\" Fraud detection systems Sistemas de detecci\u00f3n de fraude \"AI algorithms are employed in fraud detection systems to identify suspicious activities.\" Physical robots Robots f\u00edsicos \"Not all applications of AI involve physical robots ; some are purely software-based.\" Complex Complejo/a \"The relationship between robots and AI is complex and constantly evolving.\"","title":"Robots and Artificial Intelligence"},{"location":"contents/robotics/robots_ai.html#robots-and-ai","text":"","title":"Robots and AI"},{"location":"contents/robotics/robots_ai.html#robots-and-artificial-intelligence","text":"Robots and artificial intelligence (AI) are closely related, but they are not the same thing. A robot is a machine that can be programmed to perform a variety of tasks, while AI refers to the ability of machines to perform tasks that would normally require human intelligence, such as learning, problem solving, and decision making. In many cases, robots use AI to make decisions about what actions to take. For example, a robot in a manufacturing plant might use AI to decide which parts to pick up and assemble based on the shape and size of the parts. Similarly, a robot in a healthcare setting might use AI to analyze medical images and make recommendations to doctors. However, not all robots use AI. Some robots are programmed to perform a specific set of tasks without any decision making capabilities. For example, a robot used to move heavy objects in a warehouse might simply follow a predetermined path and pick up objects along the way. On the other hand, not all AI is used in robots. AI can be used in many different applications, such as virtual assistants, self-driving cars, and fraud detection systems. These applications do not necessarily involve physical robots. Overall, the relationship between robots and AI is complex and evolving. As you study robotics and AI, you'll learn more about how these technologies are used together and separately to solve real-world problems.","title":"Robots and Artificial Intelligence"},{"location":"contents/robotics/robots_ai.html#glossary","text":"English Spanish Example Sentence (English) Robots Robots \" Robots are used in manufacturing to automate repetitive tasks.\" Artificial Intelligence Inteligencia Artificial \" Artificial Intelligence enables machines to learn, reason, and make decisions.\" Closely related Estrechamente relacionados/as \" Robots and artificial intelligence are closely related in the field of robotics.\" Not the same thing No es lo mismo \" Robots and AI are not the same thing , although they have connections.\" Machine M\u00e1quina \"The robotic machine performed complex tasks with precision.\" Programmed Programado/a \"The robot was programmed to follow a specific set of instructions.\" Variety of tasks Variedad de tareas \"Robots can be programmed to perform a variety of tasks in different industries.\" Learning Aprendizaje \"Artificial Intelligence involves learning from data to improve performance.\" Problem solving Resoluci\u00f3n de problemas \"AI algorithms are capable of problem solving in various domains.\" Decision making Toma de decisiones \"Robots equipped with AI can make intelligent decision making based on sensor data.\" Manufacturing plant Planta de manufactura \"The manufacturing plant used robots to automate the assembly line.\" Parts Partes \"The robot identified and sorted the different parts in the production process.\" Shape and size Forma y tama\u00f1o \"The robot used AI to determine the appropriate shape and size of the objects to handle.\" Healthcare setting Entorno sanitario \"Robots with AI are being used in healthcare settings to assist doctors and nurses.\" Analyze Analizar \"The robot analyzed the medical images to detect anomalies and abnormalities.\" Recommendations Recomendaciones \"Based on AI analysis, the robot provided accurate recommendations for treatment options.\" Decision making capabilities Capacidad de toma de decisiones \"This robot has advanced decision making capabilities based on its AI algorithms.\" Warehouse Almac\u00e9n \"The robot efficiently navigated the warehouse to retrieve and store items.\" Predetermined path Ruta predeterminada \"The robot followed a predetermined path to perform its tasks in the factory.\" Virtual assistants Asistentes virtuales \"Virtual assistants use AI to interact with users and provide helpful information.\" Self-driving cars Coches aut\u00f3nomos \"AI technology is driving the development of self-driving cars .\" Fraud detection systems Sistemas de detecci\u00f3n de fraude \"AI algorithms are employed in fraud detection systems to identify suspicious activities.\" Physical robots Robots f\u00edsicos \"Not all applications of AI involve physical robots ; some are purely software-based.\" Complex Complejo/a \"The relationship between robots and AI is complex and constantly evolving.\"","title":"Glossary"},{"location":"contents/robotics/sensors.html","text":"Sensors Definition of sensor Sensors are devices that can detect and measure physical and environmental conditions such as temperature, light, sound, and movement. In this lesson, we will learn about the different types of sensors used in AI applications and the different ways in which they can be used. Types of sensors Optical sensors : These sensors detect light and are used in applications such as facial recognition, object detection, and image processing. Examples of optical sensors include cameras and lidar sensors. Temperature sensors : These sensors measure temperature and are used in applications such as climate control and food safety. Examples of temperature sensors include thermocouples and thermistors. Pressure sensors : These sensors measure pressure and are used in applications such as industrial automation, weather forecasting, and healthcare. Examples of pressure sensors include piezoelectric sensors and strain gauge sensors. Accelerometer sensors : These sensors measure acceleration and are used in applications such as motion detection, navigation, and gaming. Examples of accelerometer sensors include MEMS accelerometers and piezoelectric accelerometers. Gyroscopic sensors : These sensors measure angular velocity and are used in applications such as navigation, gaming, and robotics. Examples of gyroscopic sensors include MEMS gyroscopes and fiber optic gyroscopes. Magnetic sensors : These sensors measure magnetic fields and are used in applications such as navigation, industrial automation, and healthcare. Examples of magnetic sensors include Hall effect sensors and magnetoresistive sensors. Ultrasonic sensors : These sensors measure distance and are used in applications such as object detection, navigation, and industrial automation. Examples of ultrasonic sensors include sonar sensors and lidar sensors. Infrared sensors : These sensors detect infrared radiation and are used in applications such as temperature measurement, night vision, and gesture recognition. Examples of infrared sensors include thermopile sensors and pyroelectric sensors. Proximity sensors : These sensors detect the presence of objects and are used in applications such as gesture recognition, object detection, and access control. Examples of proximity sensors include infrared proximity sensors and ultrasonic proximity sensors. Light sensors : These sensors detect light and are used in applications such as light control, gesture recognition, and object detection. Examples of light sensors include photodiodes and phototransistors. Humidity sensors : These sensors measure humidity and are used in applications such as weather forecasting, agriculture, and healthcare. Examples of humidity sensors include capacitive humidity sensors and resistive humidity sensors. Gas sensors : These sensors detect the presence of gases and are used in applications such as environmental monitoring, industrial automation, and healthcare. Examples of gas sensors include electrochemical gas sensors and metal oxide gas sensors. Quiz 1. What do sensors detect and measure? a) Physical and environmental conditions b) Human emotions c) Food flavors d) Political opinions 2. Which type of sensor is used in facial recognition and image processing? a) Temperature sensors b) Pressure sensors c) Optical sensors d) Magnetic sensors 3. Which type of sensor measures acceleration? a) Temperature sensors b) Gyroscopic sensors c) Humidity sensors d) Gas sensors 4. What do proximity sensors detect? a) The presence of objects b) The color of objects c) The weight of objects d) The shape of objects 5. What type of sensors measure angular velocity and can be used in navigation and robotics? a) Gyroscopic sensors b) Temperature sensors c) Infrared sensors d) Gas sensors Glossary English Spanish Example Sentence (English) Accelerometer sensors Sensores de aceleraci\u00f3n \"The accelerometer sensors detected sudden movements and adjusted the robot's trajectory.\" Color sensors Sensores de color \"The robot's color sensors enabled it to distinguish between different objects based on hue.\" Force sensors Sensores de fuerza \"The robot used force sensors to measure the applied force during object manipulation.\" Gas sensors Sensores de gas \"The robot's safety was enhanced by gas sensors that detected hazardous fumes.\" Gyroscopic sensors Sensores girosc\u00f3picos \"The robot's precise movements were achieved with the help of gyroscopic sensors .\" Humidity sensors Sensores de humedad \"The humidity sensors ensured optimal conditions for plant growth in automated farming.\" Infrared sensors Sensores infrarrojos \"The infrared sensors detected human body heat for gesture recognition.\" Light sensors Sensores de luz \"The robot adjusted its behavior based on the readings from the light sensors .\" Magnetic sensors Sensores magn\u00e9ticos \"The robot relied on magnetic sensors to navigate and avoid obstacles in its path.\" Motion sensors Sensores de movimiento \"The robot's behavior was influenced by the readings from the motion sensors in its environment.\" Pressure sensors Sensores de presi\u00f3n \"The robot used pressure sensors to monitor the gripping force during assembly tasks.\" Proximity sensors Sensores de proximidad \"The robot's precise movements were achieved with the help of proximity sensors .\" Sound sensors Sensores de sonido \"The robot utilized sound sensors to identify specific audio patterns in its environment.\" Temperature sensors Sensores de temperatura \"The robot's temperature sensors ensured optimal conditions for storing perishable goods.\" Touch sensors Sensores t\u00e1ctiles \"The robot's touch sensors allowed it to detect and respond to human touch.\" Ultrasonic sensors Sensores ultras\u00f3nicos \"The robot used ultrasonic sensors to detect the presence of objects in its vicinity.\" Vibration sensors Sensores de vibraci\u00f3n \"The robot's navigation system incorporated vibration sensors for terrain analysis.\" GPS sensors Sensores GPS \"The robot relied on GPS sensors to navigate outdoor environments with precise location tracking.\" Humidity sensors Sensores de humedad \"The humidity sensors ensured optimal conditions for weather forecasting and agriculture.\" Infrared sensors Sensores infrarrojos \"The infrared sensors detected human body heat for gesture recognition.\" Light sensors Sensores de luz \"The robot adjusted its behavior based on the readings from the **light","title":"Robots Sensors"},{"location":"contents/robotics/sensors.html#sensors","text":"","title":"Sensors"},{"location":"contents/robotics/sensors.html#definition-of-sensor","text":"Sensors are devices that can detect and measure physical and environmental conditions such as temperature, light, sound, and movement. In this lesson, we will learn about the different types of sensors used in AI applications and the different ways in which they can be used.","title":"Definition of sensor"},{"location":"contents/robotics/sensors.html#types-of-sensors","text":"Optical sensors : These sensors detect light and are used in applications such as facial recognition, object detection, and image processing. Examples of optical sensors include cameras and lidar sensors. Temperature sensors : These sensors measure temperature and are used in applications such as climate control and food safety. Examples of temperature sensors include thermocouples and thermistors. Pressure sensors : These sensors measure pressure and are used in applications such as industrial automation, weather forecasting, and healthcare. Examples of pressure sensors include piezoelectric sensors and strain gauge sensors. Accelerometer sensors : These sensors measure acceleration and are used in applications such as motion detection, navigation, and gaming. Examples of accelerometer sensors include MEMS accelerometers and piezoelectric accelerometers. Gyroscopic sensors : These sensors measure angular velocity and are used in applications such as navigation, gaming, and robotics. Examples of gyroscopic sensors include MEMS gyroscopes and fiber optic gyroscopes. Magnetic sensors : These sensors measure magnetic fields and are used in applications such as navigation, industrial automation, and healthcare. Examples of magnetic sensors include Hall effect sensors and magnetoresistive sensors. Ultrasonic sensors : These sensors measure distance and are used in applications such as object detection, navigation, and industrial automation. Examples of ultrasonic sensors include sonar sensors and lidar sensors. Infrared sensors : These sensors detect infrared radiation and are used in applications such as temperature measurement, night vision, and gesture recognition. Examples of infrared sensors include thermopile sensors and pyroelectric sensors. Proximity sensors : These sensors detect the presence of objects and are used in applications such as gesture recognition, object detection, and access control. Examples of proximity sensors include infrared proximity sensors and ultrasonic proximity sensors. Light sensors : These sensors detect light and are used in applications such as light control, gesture recognition, and object detection. Examples of light sensors include photodiodes and phototransistors. Humidity sensors : These sensors measure humidity and are used in applications such as weather forecasting, agriculture, and healthcare. Examples of humidity sensors include capacitive humidity sensors and resistive humidity sensors. Gas sensors : These sensors detect the presence of gases and are used in applications such as environmental monitoring, industrial automation, and healthcare. Examples of gas sensors include electrochemical gas sensors and metal oxide gas sensors.","title":"Types of sensors"},{"location":"contents/robotics/sensors.html#quiz","text":"1. What do sensors detect and measure? a) Physical and environmental conditions b) Human emotions c) Food flavors d) Political opinions 2. Which type of sensor is used in facial recognition and image processing? a) Temperature sensors b) Pressure sensors c) Optical sensors d) Magnetic sensors 3. Which type of sensor measures acceleration? a) Temperature sensors b) Gyroscopic sensors c) Humidity sensors d) Gas sensors 4. What do proximity sensors detect? a) The presence of objects b) The color of objects c) The weight of objects d) The shape of objects 5. What type of sensors measure angular velocity and can be used in navigation and robotics? a) Gyroscopic sensors b) Temperature sensors c) Infrared sensors d) Gas sensors","title":"Quiz"},{"location":"contents/robotics/sensors.html#glossary","text":"English Spanish Example Sentence (English) Accelerometer sensors Sensores de aceleraci\u00f3n \"The accelerometer sensors detected sudden movements and adjusted the robot's trajectory.\" Color sensors Sensores de color \"The robot's color sensors enabled it to distinguish between different objects based on hue.\" Force sensors Sensores de fuerza \"The robot used force sensors to measure the applied force during object manipulation.\" Gas sensors Sensores de gas \"The robot's safety was enhanced by gas sensors that detected hazardous fumes.\" Gyroscopic sensors Sensores girosc\u00f3picos \"The robot's precise movements were achieved with the help of gyroscopic sensors .\" Humidity sensors Sensores de humedad \"The humidity sensors ensured optimal conditions for plant growth in automated farming.\" Infrared sensors Sensores infrarrojos \"The infrared sensors detected human body heat for gesture recognition.\" Light sensors Sensores de luz \"The robot adjusted its behavior based on the readings from the light sensors .\" Magnetic sensors Sensores magn\u00e9ticos \"The robot relied on magnetic sensors to navigate and avoid obstacles in its path.\" Motion sensors Sensores de movimiento \"The robot's behavior was influenced by the readings from the motion sensors in its environment.\" Pressure sensors Sensores de presi\u00f3n \"The robot used pressure sensors to monitor the gripping force during assembly tasks.\" Proximity sensors Sensores de proximidad \"The robot's precise movements were achieved with the help of proximity sensors .\" Sound sensors Sensores de sonido \"The robot utilized sound sensors to identify specific audio patterns in its environment.\" Temperature sensors Sensores de temperatura \"The robot's temperature sensors ensured optimal conditions for storing perishable goods.\" Touch sensors Sensores t\u00e1ctiles \"The robot's touch sensors allowed it to detect and respond to human touch.\" Ultrasonic sensors Sensores ultras\u00f3nicos \"The robot used ultrasonic sensors to detect the presence of objects in its vicinity.\" Vibration sensors Sensores de vibraci\u00f3n \"The robot's navigation system incorporated vibration sensors for terrain analysis.\" GPS sensors Sensores GPS \"The robot relied on GPS sensors to navigate outdoor environments with precise location tracking.\" Humidity sensors Sensores de humedad \"The humidity sensors ensured optimal conditions for weather forecasting and agriculture.\" Infrared sensors Sensores infrarrojos \"The infrared sensors detected human body heat for gesture recognition.\" Light sensors Sensores de luz \"The robot adjusted its behavior based on the readings from the **light","title":"Glossary"},{"location":"pd/curriculo/curriculo.html","text":"Elementos curriculares Competencias clave CCL : competencia en comunicaci\u00f3n ling\u00fc\u00edstica CP : competencia pluriling\u00fce CMCT : competencia matem\u00e1tica, ciencia y tecnol\u00f3gica CD : competencia digital CPSAA : competencia personal, social y de aprender a aprender CC : competencia ciudadana CE : competencia emprendedora CCEC : competencia en conciencia y expresi\u00f3n cultural Competencias espec\u00edficas CE1 . Identificar, investigar y emplear t\u00e9cnicas de inteligencia artificial y virtualizaci\u00f3n de la realidad en el abordaje y la b\u00fasqueda de soluciones a problemas b\u00e1sicos de la sociedad valorando los principios \u00e9ticos e inclusivos aplicados. CE2 . Aplicar el pensamiento computacional en el an\u00e1lisis y resoluci\u00f3n de problemas b\u00e1sicos y significativos para el alumnado mediante el desarrollo de software. CE3 . Montar sistemas rob\u00f3ticos sencillos, analizando las respuestas que proporcionan en su interacci\u00f3n con el entorno y valorando la eficacia de estas frente a los retos planteados. CE4 Afrontar retos tecnol\u00f3gicos sencillos y proponer soluciones mediante la programaci\u00f3n, la inteligencia Artificial y la rob\u00f3tica, analizando las posibilidades y valorando cr\u00edticamente las implicaciones \u00e9ticas y ecosociales. Relaciones o conexiones con las competencias clave CE1: CCL, CP, CMCT, CD, CPSAA CE2: CCL, CMCT, CD, CPSAA CE3: CMCT, CD, CPSAA CE4: CMCT, CD, CPSAA, CC, CE Saberes b\u00e1sicos Bloque 1. Inteligencia Artifical. CE1 Sensores, tipolog\u00eda y aplicaciones. T\u00e9cnicas iniciales de IA: sistemas expertos, redes neuronales y aprendizaje autom\u00e1tico. Procesado autom\u00e1tico de la informaci\u00f3n. Equidad e inclusi\u00f3n en sistemas de IA. Sesgos en IA. Implicaciones sociales y \u00e9ticas de la inteligencia artificial. T\u00e9cnicas de virtualizaci\u00f3n de la realidad. Bloque 2. Programaci\u00f3n. CE2 Interpretaci\u00f3n de la realidad mediante modelado de problemas. Abstracci\u00f3n, secuenciaci\u00f3n, algor\u00edtmica y su representaci\u00f3n con lenguaje natural y diagramas de flujo. Detecci\u00f3n y reutilizaci\u00f3n de patrones. Generalizaci\u00f3n. Sostenibilidad e inclusi\u00f3n como requisitos del dise\u00f1o del software. Estructuras de control del flujo del programa. Variables, constantes, condiciones y operadores. Introducci\u00f3n a la programaci\u00f3n en lenguajes de alto nivel. Tipos de lenguajes. Sintaxis y sem\u00e1ntica Programaci\u00f3n de aplicaciones para dispositivos m\u00f3viles. Evaluaci\u00f3n y mantenimiento de software. Licencias de software. El software libre y el software propietario. Simuladores de tarjetas controladoras. Iniciativa, autoconfianza y metacognici\u00f3n en el proceso de aprendizaje del desarrollo de software. Bloque 3. Rob\u00f3tica. CE3 Montaje de robots. Control de sistemas robotizados. Sensores, actuadores y controladores. Carga y ejecuci\u00f3n de los algoritmos en robots. Sistemas robotizados en la experimentaci\u00f3n con prototipos dise\u00f1ados. Situaciones de aprendizaje texto. Criterios de evaluaci\u00f3n Competencia espec\u00edfica 1 CE1 . Identificar, investigar y emplear t\u00e9cnicas de inteligencia artificial y virtualizaci\u00f3n de la realidad en el abordaje y la b\u00fasqueda de soluciones a problemas b\u00e1sicos de la sociedad valorando los principios \u00e9ticos e inclusivos aplicados. 1.1. Identificar el funcionamiento de t\u00e9cnicas de IA. 1.2. Investigar situaciones donde se aplican t\u00e9cnicas de IA. 1.3. Valorar criterios \u00e9ticos aplicados a las funciones de IA. 1.4. Emplear funciones de IA en aplicaciones sencillas siguiendo criterios \u00e9ticos e inclusivos para buscar soluciones a problemas b\u00e1sicos 1.5 Emplear t\u00e9cnicas sencillas de virtualizaci\u00f3n de la realidad. Competencia espec\u00edfica 2 CE2 . Aplicar el pensamiento computacional en el an\u00e1lisis y resoluci\u00f3n de problemas b\u00e1sicos y significativos para el alumnado mediante el desarrollo de software. 2.1. Analizar problemas b\u00e1sicos significativos para el alumnado, mediante el uso de las estructuras de control m\u00e1s adecuadas. 2.2. Evaluar y mantener las aplicaciones inform\u00e1ticas desarrolladas por el propio alumnado. 2.3. Planificar de forma aut\u00f3noma la soluci\u00f3n de problemas b\u00e1sicos, utilizando los algoritmos y las estructuras de datos m\u00e1s adecuados. 2.4. Programar aplicaciones sencillas multiplataforma de manera aut\u00f3noma para resolver problemas b\u00e1sicos. 2.5. Aplicar y respetar los derechos de autor\u00eda, licencias de derechos y explotaci\u00f3n durante la creaci\u00f3n de software. Competencia espec\u00edfica 3 CE3 . Montar sistemas rob\u00f3ticos sencillos, analizando las respuestas que proporcionan en su interacci\u00f3n con el entorno y valorando la eficacia de estas frente a los retos planteados. 3.1. Montar robots de mayor complejidad empleando sensores, actuadores y otros operadores. 3.2. Conectar, transferir y validar la ejecuci\u00f3n del programa de control seleccionado al robot. 3.3. Seleccionar los m\u00f3dulos de entrada y salida para montar robots sencillos, que sean capaces de realizar tareas de forma aut\u00f3noma. 3.4. Analizar y evaluar la eficacia de la interacci\u00f3n del robot con el entorno. 3.5. Programar instrucciones sencillas multiplataforma de manera aut\u00f3noma para controlar un robot programable. 3.6. Controlar el robot por parte del usuario en tiempo real y de forma remota. Competencia espec\u00edfica 4 CE4 Afrontar retos tecnol\u00f3gicos sencillos y proponer soluciones mediante la programaci\u00f3n, la inteligencia Artificial y la rob\u00f3tica, analizando las posibilidades y valorando cr\u00edticamente las implicaciones \u00e9ticas y ecosociales. 4.1. Planificar tareas sencillas, crear estructuras de equipos de trabajo, distribuir funciones y responsabilidades de las personas integrantes y colaborar proactivamente en el desarrollo de soluciones digitales y tecnol\u00f3gicas 4.2. Valorar la importancia de la Inteligencia Artificial, la programaci\u00f3n y la rob\u00f3tica como elementos disruptores de la transformaci\u00f3n social, cultural y cient\u00edfica actuales. 4.3. Dise\u00f1ar soluciones utilizando la programaci\u00f3n, la inteligencia artificial y la rob\u00f3tica eligiendo la opci\u00f3n que mejor se adapte a los retos planteados. 4.4. Gestionar situaciones de incertidumbre en entornos digitales y tecnol\u00f3gicos con una actitud positiva, y afrontarlas utilizando el conocimiento adquirido y sinti\u00e9ndose competente. 4.5. Aplicar la sostenibilidad e inclusi\u00f3n como requisitos del dise\u00f1o de soluciones tecnol\u00f3gicas","title":"Elementos curriculares"},{"location":"pd/curriculo/curriculo.html#elementos-curriculares","text":"","title":"Elementos curriculares"},{"location":"pd/curriculo/curriculo.html#competencias-clave","text":"CCL : competencia en comunicaci\u00f3n ling\u00fc\u00edstica CP : competencia pluriling\u00fce CMCT : competencia matem\u00e1tica, ciencia y tecnol\u00f3gica CD : competencia digital CPSAA : competencia personal, social y de aprender a aprender CC : competencia ciudadana CE : competencia emprendedora CCEC : competencia en conciencia y expresi\u00f3n cultural","title":"Competencias clave"},{"location":"pd/curriculo/curriculo.html#competencias-especificas","text":"CE1 . Identificar, investigar y emplear t\u00e9cnicas de inteligencia artificial y virtualizaci\u00f3n de la realidad en el abordaje y la b\u00fasqueda de soluciones a problemas b\u00e1sicos de la sociedad valorando los principios \u00e9ticos e inclusivos aplicados. CE2 . Aplicar el pensamiento computacional en el an\u00e1lisis y resoluci\u00f3n de problemas b\u00e1sicos y significativos para el alumnado mediante el desarrollo de software. CE3 . Montar sistemas rob\u00f3ticos sencillos, analizando las respuestas que proporcionan en su interacci\u00f3n con el entorno y valorando la eficacia de estas frente a los retos planteados. CE4 Afrontar retos tecnol\u00f3gicos sencillos y proponer soluciones mediante la programaci\u00f3n, la inteligencia Artificial y la rob\u00f3tica, analizando las posibilidades y valorando cr\u00edticamente las implicaciones \u00e9ticas y ecosociales.","title":"Competencias espec\u00edficas"},{"location":"pd/curriculo/curriculo.html#relaciones-o-conexiones-con-las-competencias-clave","text":"CE1: CCL, CP, CMCT, CD, CPSAA CE2: CCL, CMCT, CD, CPSAA CE3: CMCT, CD, CPSAA CE4: CMCT, CD, CPSAA, CC, CE","title":"Relaciones o conexiones con las competencias clave"},{"location":"pd/curriculo/curriculo.html#saberes-basicos","text":"","title":"Saberes b\u00e1sicos"},{"location":"pd/curriculo/curriculo.html#bloque-1-inteligencia-artifical-ce1","text":"Sensores, tipolog\u00eda y aplicaciones. T\u00e9cnicas iniciales de IA: sistemas expertos, redes neuronales y aprendizaje autom\u00e1tico. Procesado autom\u00e1tico de la informaci\u00f3n. Equidad e inclusi\u00f3n en sistemas de IA. Sesgos en IA. Implicaciones sociales y \u00e9ticas de la inteligencia artificial. T\u00e9cnicas de virtualizaci\u00f3n de la realidad.","title":"Bloque 1. Inteligencia Artifical. CE1"},{"location":"pd/curriculo/curriculo.html#bloque-2-programacion-ce2","text":"Interpretaci\u00f3n de la realidad mediante modelado de problemas. Abstracci\u00f3n, secuenciaci\u00f3n, algor\u00edtmica y su representaci\u00f3n con lenguaje natural y diagramas de flujo. Detecci\u00f3n y reutilizaci\u00f3n de patrones. Generalizaci\u00f3n. Sostenibilidad e inclusi\u00f3n como requisitos del dise\u00f1o del software. Estructuras de control del flujo del programa. Variables, constantes, condiciones y operadores. Introducci\u00f3n a la programaci\u00f3n en lenguajes de alto nivel. Tipos de lenguajes. Sintaxis y sem\u00e1ntica Programaci\u00f3n de aplicaciones para dispositivos m\u00f3viles. Evaluaci\u00f3n y mantenimiento de software. Licencias de software. El software libre y el software propietario. Simuladores de tarjetas controladoras. Iniciativa, autoconfianza y metacognici\u00f3n en el proceso de aprendizaje del desarrollo de software.","title":"Bloque 2. Programaci\u00f3n. CE2"},{"location":"pd/curriculo/curriculo.html#bloque-3-robotica-ce3","text":"Montaje de robots. Control de sistemas robotizados. Sensores, actuadores y controladores. Carga y ejecuci\u00f3n de los algoritmos en robots. Sistemas robotizados en la experimentaci\u00f3n con prototipos dise\u00f1ados.","title":"Bloque 3. Rob\u00f3tica. CE3"},{"location":"pd/curriculo/curriculo.html#situaciones-de-aprendizaje","text":"texto.","title":"Situaciones de aprendizaje"},{"location":"pd/curriculo/curriculo.html#criterios-de-evaluacion","text":"","title":"Criterios de evaluaci\u00f3n"},{"location":"pd/curriculo/curriculo.html#competencia-especifica-1","text":"CE1 . Identificar, investigar y emplear t\u00e9cnicas de inteligencia artificial y virtualizaci\u00f3n de la realidad en el abordaje y la b\u00fasqueda de soluciones a problemas b\u00e1sicos de la sociedad valorando los principios \u00e9ticos e inclusivos aplicados. 1.1. Identificar el funcionamiento de t\u00e9cnicas de IA. 1.2. Investigar situaciones donde se aplican t\u00e9cnicas de IA. 1.3. Valorar criterios \u00e9ticos aplicados a las funciones de IA. 1.4. Emplear funciones de IA en aplicaciones sencillas siguiendo criterios \u00e9ticos e inclusivos para buscar soluciones a problemas b\u00e1sicos 1.5 Emplear t\u00e9cnicas sencillas de virtualizaci\u00f3n de la realidad.","title":"Competencia espec\u00edfica 1"},{"location":"pd/curriculo/curriculo.html#competencia-especifica-2","text":"CE2 . Aplicar el pensamiento computacional en el an\u00e1lisis y resoluci\u00f3n de problemas b\u00e1sicos y significativos para el alumnado mediante el desarrollo de software. 2.1. Analizar problemas b\u00e1sicos significativos para el alumnado, mediante el uso de las estructuras de control m\u00e1s adecuadas. 2.2. Evaluar y mantener las aplicaciones inform\u00e1ticas desarrolladas por el propio alumnado. 2.3. Planificar de forma aut\u00f3noma la soluci\u00f3n de problemas b\u00e1sicos, utilizando los algoritmos y las estructuras de datos m\u00e1s adecuados. 2.4. Programar aplicaciones sencillas multiplataforma de manera aut\u00f3noma para resolver problemas b\u00e1sicos. 2.5. Aplicar y respetar los derechos de autor\u00eda, licencias de derechos y explotaci\u00f3n durante la creaci\u00f3n de software.","title":"Competencia espec\u00edfica 2"},{"location":"pd/curriculo/curriculo.html#competencia-especifica-3","text":"CE3 . Montar sistemas rob\u00f3ticos sencillos, analizando las respuestas que proporcionan en su interacci\u00f3n con el entorno y valorando la eficacia de estas frente a los retos planteados. 3.1. Montar robots de mayor complejidad empleando sensores, actuadores y otros operadores. 3.2. Conectar, transferir y validar la ejecuci\u00f3n del programa de control seleccionado al robot. 3.3. Seleccionar los m\u00f3dulos de entrada y salida para montar robots sencillos, que sean capaces de realizar tareas de forma aut\u00f3noma. 3.4. Analizar y evaluar la eficacia de la interacci\u00f3n del robot con el entorno. 3.5. Programar instrucciones sencillas multiplataforma de manera aut\u00f3noma para controlar un robot programable. 3.6. Controlar el robot por parte del usuario en tiempo real y de forma remota.","title":"Competencia espec\u00edfica 3"},{"location":"pd/curriculo/curriculo.html#competencia-especifica-4","text":"CE4 Afrontar retos tecnol\u00f3gicos sencillos y proponer soluciones mediante la programaci\u00f3n, la inteligencia Artificial y la rob\u00f3tica, analizando las posibilidades y valorando cr\u00edticamente las implicaciones \u00e9ticas y ecosociales. 4.1. Planificar tareas sencillas, crear estructuras de equipos de trabajo, distribuir funciones y responsabilidades de las personas integrantes y colaborar proactivamente en el desarrollo de soluciones digitales y tecnol\u00f3gicas 4.2. Valorar la importancia de la Inteligencia Artificial, la programaci\u00f3n y la rob\u00f3tica como elementos disruptores de la transformaci\u00f3n social, cultural y cient\u00edfica actuales. 4.3. Dise\u00f1ar soluciones utilizando la programaci\u00f3n, la inteligencia artificial y la rob\u00f3tica eligiendo la opci\u00f3n que mejor se adapte a los retos planteados. 4.4. Gestionar situaciones de incertidumbre en entornos digitales y tecnol\u00f3gicos con una actitud positiva, y afrontarlas utilizando el conocimiento adquirido y sinti\u00e9ndose competente. 4.5. Aplicar la sostenibilidad e inclusi\u00f3n como requisitos del dise\u00f1o de soluciones tecnol\u00f3gicas","title":"Competencia espec\u00edfica 4"},{"location":"pd/progaula/progaula.html","text":"Programaci\u00f3n de aula Method Description GET Fetch resource PUT Update resource DELETE Delete resource graph LR A[Start] --> B{Error?}; B -->|Yes| C[Hmm...]; C --> D[Debug]; D --> B; B ---->|No| E[Yay!]; sequenceDiagram autonumber Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! sequenceDiagram Alice->>John: Hello John, how are you? John-->>Alice: Great! Alice-)John: See you later! sequenceDiagram participant A as Alice participant J as John A->>J: Hello John, how are you? J->>A: Great!","title":"Programaci\u00f3n de aula"},{"location":"pd/progaula/progaula.html#programacion-de-aula","text":"Method Description GET Fetch resource PUT Update resource DELETE Delete resource graph LR A[Start] --> B{Error?}; B -->|Yes| C[Hmm...]; C --> D[Debug]; D --> B; B ---->|No| E[Yay!]; sequenceDiagram autonumber Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! sequenceDiagram Alice->>John: Hello John, how are you? John-->>Alice: Great! Alice-)John: See you later! sequenceDiagram participant A as Alice participant J as John A->>J: Hello John, how are you? J->>A: Great!","title":"Programaci\u00f3n de aula"}]}